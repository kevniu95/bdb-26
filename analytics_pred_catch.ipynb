{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1ece4cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.cluster import KMeans\n",
    "from multiprocessing import Pool as MultiprocessingPool, cpu_count\n",
    "\n",
    "\n",
    "from src.kinematics import calculate_speed_and_direction\n",
    "# from kinematics import add_kinematics\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cbbc19b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIG\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    DATA_DIR = Path(\"./data\")\n",
    "    OUTPUT_DIR = Path(\"./outputs\")\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    SEED = 42\n",
    "    N_FOLDS = 5\n",
    "    BATCH_SIZE = 256\n",
    "    EPOCHS = 200\n",
    "    PATIENCE = 30\n",
    "    LEARNING_RATE = 1e-4\n",
    "    \n",
    "    WINDOW_SIZE = 10\n",
    "    HIDDEN_DIM = 128\n",
    "    MAX_FUTURE_HORIZON = 94\n",
    "    \n",
    "    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n",
    "    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n",
    "    \n",
    "    K_NEIGH = 6\n",
    "    RADIUS = 30.0\n",
    "    TAU = 8.0\n",
    "    N_ROUTE_CLUSTERS = 7\n",
    "    \n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(Config.SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6162db51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/4] Loading data...\n",
      "‚úì Train output: (562936, 6), unique plays: 14108\n",
      "‚úì Test template: (5837, 5), unique plays: 143\n",
      "‚úì Supplementary data: (18009, 41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_m/rvnpg_cs6xzcz0vlml0lzkbm0000gp/T/ipykernel_77175/999630618.py:12: DtypeWarning: Columns (25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  supplementary_data = pd.read_csv(config.DATA_DIR / \"supplementary_data.csv\")\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "config\n",
    "\n",
    "# Load\n",
    "print(\"\\n[1/4] Loading data...\")\n",
    "train_input_files = [config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\n",
    "train_output_files = [config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n",
    "train_input = pd.concat([pd.read_csv(f) for f in train_input_files if f.exists()])\n",
    "train_output = pd.concat([pd.read_csv(f) for f in train_output_files if f.exists()])\n",
    "# test_input = pd.read_csv(config.DATA_DIR / \"test_input.csv\")\n",
    "test_template = pd.read_csv(config.DATA_DIR / \"test.csv\")\n",
    "supplementary_data = pd.read_csv(config.DATA_DIR / \"supplementary_data.csv\")\n",
    "\n",
    "# print(f\"‚úì Train input: {train_input.shape}, Train output: {train_output.shape}\")\n",
    "print(f\"‚úì Train output: {train_output.shape}, unique plays: {train_output[['game_id','play_id']].drop_duplicates().shape[0]}\")\n",
    "print(f\"‚úì Test template: {test_template.shape}, unique plays: {test_template[['game_id','play_id']].drop_duplicates().shape[0]}\")\n",
    "# print(f\"‚úì Test input: {test_input.shape}, Test template: {test_template.shape}\")\n",
    "print(f\"‚úì Supplementary data: {supplementary_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cc173424",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_results = supplementary_data[['game_id','play_id','pass_result']].drop_duplicates()\n",
    "play_results.loc[play_results['pass_result'] == 'IN', 'pass_result'] = 'I'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bfd22288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s</th>\n",
       "      <th>s_kinematics</th>\n",
       "      <th>dir</th>\n",
       "      <th>dir_kinematics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998408</td>\n",
       "      <td>-0.002528</td>\n",
       "      <td>0.034965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s_kinematics</th>\n",
       "      <td>0.998408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002918</td>\n",
       "      <td>0.034257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dir</th>\n",
       "      <td>-0.002528</td>\n",
       "      <td>-0.002918</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.931304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dir_kinematics</th>\n",
       "      <td>0.034965</td>\n",
       "      <td>0.034257</td>\n",
       "      <td>0.931304</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       s  s_kinematics       dir  dir_kinematics\n",
       "s               1.000000      0.998408 -0.002528        0.034965\n",
       "s_kinematics    0.998408      1.000000 -0.002918        0.034257\n",
       "dir            -0.002528     -0.002918  1.000000        0.931304\n",
       "dir_kinematics  0.034965      0.034257  0.931304        1.000000"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===================\n",
    "# Conduct kinematics test - NOTE: move this later\n",
    "# ===================\n",
    "ids = [\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"]\n",
    "unique_plays = train_input[['game_id', 'play_id']].drop_duplicates()\n",
    "sampled_plays = unique_plays.sample(n=2000, random_state=42)\n",
    "test = train_input.merge(sampled_plays, on=['game_id', 'play_id']).reset_index(drop=True)\n",
    "\n",
    "kinematics_test = calculate_speed_and_direction(test[['game_id', 'play_id', 'nfl_id', 'frame_id', 'x', 'y']])\n",
    "test_with_kinematics = test.merge(kinematics_test[ids + ['s','dir']], on=['game_id', 'play_id', 'nfl_id', \"frame_id\"], suffixes = ('', '_kinematics'))\n",
    "correlations = test_with_kinematics[['s', 's_kinematics', 'dir', 'dir_kinematics']].corr()\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "51fc7a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Prepare Sequences Geometric - Base\n",
    "# ==========================================\n",
    "\n",
    "# TODO: implement kinematics based on what's below\n",
    "need_to_calculate = [\"s\", \"dir\"]\n",
    "train_output = calculate_speed_and_direction(train_output)\n",
    "\n",
    "# TODO: Re-attach play features from input_df onto output_df\n",
    "keys = [\"game_id\", \"play_id\", \"nfl_id\"]\n",
    "play_features = [\"player_height\", \"player_weight\",\"player_side\",\"player_role\",\n",
    "                 \"player_position\",\n",
    "                 \"play_direction\", \"absolute_yardline_number\",\n",
    "                 \"ball_land_x\",\"ball_land_y\"]\n",
    "\n",
    "\n",
    "# TODO: Actually create the features\n",
    "input = train_input[keys + play_features].drop_duplicates()\n",
    "\n",
    "train_output = train_output.merge(input, on=keys, how='inner')\n",
    "train_output = train_output.merge(play_results, on=['game_id','play_id'], how='left', indicator= True)\n",
    "assert all(train_output['_merge'] == 'both')\n",
    "train_output = train_output.drop(columns=['_merge'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "12ae0520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GEOMETRIC BASELINE - THE BREAKTHROUGH\n",
    "# ============================================================================\n",
    "\n",
    "def compute_geometric_endpoint(df):\n",
    "    \"\"\"\n",
    "    Compute where each player SHOULD end up based on geometry.\n",
    "    This is the deterministic part - no learning needed.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Time to play end\n",
    "    if 'num_frames_output' in df.columns:\n",
    "        t_total = df['num_frames_output'] / 10.0\n",
    "    else:\n",
    "        t_total = 3.0\n",
    "    \n",
    "    df['time_to_endpoint'] = t_total\n",
    "    \n",
    "    # Initialize with momentum (default rule)\n",
    "    df['geo_endpoint_x'] = df['x'] + df['velocity_x'] * t_total\n",
    "    df['geo_endpoint_y'] = df['y'] + df['velocity_y'] * t_total\n",
    "    \n",
    "    # Rule 1: Targeted Receivers converge to ball\n",
    "    if 'ball_land_x' in df.columns:\n",
    "        receiver_mask = df['player_role'] == 'Targeted Receiver'\n",
    "        df.loc[receiver_mask, 'geo_endpoint_x'] = df.loc[receiver_mask, 'ball_land_x']\n",
    "        df.loc[receiver_mask, 'geo_endpoint_y'] = df.loc[receiver_mask, 'ball_land_y']\n",
    "        \n",
    "        # Rule 2: Defenders mirror receivers (maintain offset)\n",
    "        defender_mask = df['player_role'] == 'Defensive Coverage'\n",
    "        has_mirror = df.get('mirror_offset_x', 0).notna() & (df.get('mirror_wr_dist', 50) < 15)\n",
    "        coverage_mask = defender_mask & has_mirror\n",
    "        \n",
    "        df.loc[coverage_mask, 'geo_endpoint_x'] = (\n",
    "            df.loc[coverage_mask, 'ball_land_x'] + \n",
    "            df.loc[coverage_mask, 'mirror_offset_x'].fillna(0)\n",
    "        )\n",
    "        df.loc[coverage_mask, 'geo_endpoint_y'] = (\n",
    "            df.loc[coverage_mask, 'ball_land_y'] + \n",
    "            df.loc[coverage_mask, 'mirror_offset_y'].fillna(0)\n",
    "        )\n",
    "    \n",
    "    # Clip to field\n",
    "    df['geo_endpoint_x'] = df['geo_endpoint_x'].clip(Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n",
    "    df['geo_endpoint_y'] = df['geo_endpoint_y'].clip(Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_geometric_features(df):\n",
    "    \"\"\"Add features that describe the geometric solution\"\"\"\n",
    "    df = compute_geometric_endpoint(df)\n",
    "    \n",
    "    # Vector to geometric endpoint\n",
    "    df['geo_vector_x'] = df['geo_endpoint_x'] - df['x']\n",
    "    df['geo_vector_y'] = df['geo_endpoint_y'] - df['y']\n",
    "    df['geo_distance'] = np.sqrt(df['geo_vector_x']**2 + df['geo_vector_y']**2)\n",
    "    \n",
    "    # Required velocity to reach geometric endpoint\n",
    "    t = df['time_to_endpoint'] + 0.1\n",
    "    df['geo_required_vx'] = df['geo_vector_x'] / t\n",
    "    df['geo_required_vy'] = df['geo_vector_y'] / t\n",
    "    \n",
    "    # Current velocity vs required\n",
    "    df['geo_velocity_error_x'] = df['geo_required_vx'] - df['velocity_x']\n",
    "    df['geo_velocity_error_y'] = df['geo_required_vy'] - df['velocity_y']\n",
    "    df['geo_velocity_error'] = np.sqrt(\n",
    "        df['geo_velocity_error_x']**2 + df['geo_velocity_error_y']**2\n",
    "    )\n",
    "    \n",
    "    # Required constant acceleration (a = 2*Œîx/t¬≤)\n",
    "    t_sq = t * t\n",
    "    df['geo_required_ax'] = 2 * df['geo_vector_x'] / t_sq\n",
    "    df['geo_required_ay'] = 2 * df['geo_vector_y'] / t_sq\n",
    "    df['geo_required_ax'] = df['geo_required_ax'].clip(-10, 10)\n",
    "    df['geo_required_ay'] = df['geo_required_ay'].clip(-10, 10)\n",
    "    \n",
    "    # Alignment with geometric path\n",
    "    velocity_mag = np.sqrt(df['velocity_x']**2 + df['velocity_y']**2)\n",
    "    geo_unit_x = df['geo_vector_x'] / (df['geo_distance'] + 0.1)\n",
    "    geo_unit_y = df['geo_vector_y'] / (df['geo_distance'] + 0.1)\n",
    "    df['geo_alignment'] = (\n",
    "        df['velocity_x'] * geo_unit_x + df['velocity_y'] * geo_unit_y\n",
    "    ) / (velocity_mag + 0.1)\n",
    "    \n",
    "    # Role-specific geometric quality\n",
    "    df['geo_receiver_urgency'] = df['is_receiver'] * df['geo_distance'] / (t + 0.1)\n",
    "    df['geo_defender_coupling'] = df['is_coverage'] * (1.0 / (df.get('mirror_wr_dist', 50) + 1.0))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "be1cf131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_velocity(speed, direction_deg):\n",
    "    theta = np.deg2rad(direction_deg)\n",
    "    return speed * np.sin(theta), speed * np.cos(theta)\n",
    "\n",
    "\n",
    "def height_to_feet(height_str):\n",
    "    try:\n",
    "        ft, inches = map(int, str(height_str).split('-'))\n",
    "        return ft + inches/12\n",
    "    except:\n",
    "        return 6.0\n",
    "\n",
    "def get_opponent_features(input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Enhanced opponent interaction with MIRROR WR tracking\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for (gid, pid), group in tqdm(input_df.groupby(['game_id', 'play_id']), \n",
    "                                   desc=\"üèà Opponents\", leave=False):\n",
    "        last = group.sort_values('frame_id').groupby('nfl_id').last()\n",
    "        \n",
    "        if len(last) < 2:\n",
    "            continue\n",
    "            \n",
    "        positions = last[['x', 'y']].values\n",
    "        sides = last['player_side'].values\n",
    "        speeds = last['s'].values\n",
    "        directions = last['dir'].values\n",
    "        roles = last['player_role'].values\n",
    "        \n",
    "        receiver_mask = np.isin(roles, ['Targeted Receiver', 'Other Route Runner'])\n",
    "        \n",
    "        for i, (nid, side, role) in enumerate(zip(last.index, sides, roles)):\n",
    "            opp_mask = sides != side\n",
    "            \n",
    "            feat = {\n",
    "                'game_id': gid, 'play_id': pid, 'nfl_id': nid,\n",
    "                'nearest_opp_dist': 50.0, 'closing_speed': 0.0,\n",
    "                'num_nearby_opp_3': 0, 'num_nearby_opp_5': 0,\n",
    "                'mirror_wr_vx': 0.0, 'mirror_wr_vy': 0.0,\n",
    "                'mirror_offset_x': 0.0, 'mirror_offset_y': 0.0,\n",
    "                'mirror_wr_dist': 50.0,\n",
    "            }\n",
    "            \n",
    "            if not opp_mask.any():\n",
    "                features.append(feat)\n",
    "                continue\n",
    "            \n",
    "            opp_positions = positions[opp_mask]\n",
    "            distances = np.sqrt(((positions[i] - opp_positions)**2).sum(axis=1))\n",
    "            \n",
    "            if len(distances) == 0:\n",
    "                features.append(feat)\n",
    "                continue\n",
    "                \n",
    "            nearest_idx = distances.argmin()\n",
    "            feat['nearest_opp_dist'] = distances[nearest_idx]\n",
    "            feat['num_nearby_opp_3'] = (distances < 3.0).sum()\n",
    "            feat['num_nearby_opp_5'] = (distances < 5.0).sum()\n",
    "            \n",
    "            my_vx, my_vy = get_velocity(speeds[i], directions[i])\n",
    "            opp_speeds = speeds[opp_mask]\n",
    "            opp_dirs = directions[opp_mask]\n",
    "            opp_vx, opp_vy = get_velocity(opp_speeds[nearest_idx], opp_dirs[nearest_idx])\n",
    "            \n",
    "            rel_vx = my_vx - opp_vx\n",
    "            rel_vy = my_vy - opp_vy\n",
    "            to_me = positions[i] - opp_positions[nearest_idx]\n",
    "            to_me_norm = to_me / (np.linalg.norm(to_me) + 0.1)\n",
    "            feat['closing_speed'] = -(rel_vx * to_me_norm[0] + rel_vy * to_me_norm[1])\n",
    "            \n",
    "            if role == 'Defensive Coverage' and receiver_mask.any():\n",
    "                rec_positions = positions[receiver_mask]\n",
    "                rec_distances = np.sqrt(((positions[i] - rec_positions)**2).sum(axis=1))\n",
    "                \n",
    "                if len(rec_distances) > 0:\n",
    "                    closest_rec_idx = rec_distances.argmin()\n",
    "                    rec_indices = np.where(receiver_mask)[0]\n",
    "                    actual_rec_idx = rec_indices[closest_rec_idx]\n",
    "                    \n",
    "                    rec_vx, rec_vy = get_velocity(speeds[actual_rec_idx], directions[actual_rec_idx])\n",
    "                    \n",
    "                    feat['mirror_wr_vx'] = rec_vx\n",
    "                    feat['mirror_wr_vy'] = rec_vy\n",
    "                    feat['mirror_wr_dist'] = rec_distances[closest_rec_idx]\n",
    "                    feat['mirror_offset_x'] = positions[i][0] - rec_positions[closest_rec_idx][0]\n",
    "                    feat['mirror_offset_y'] = positions[i][1] - rec_positions[closest_rec_idx][1]\n",
    "            \n",
    "            features.append(feat)\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "def extract_route_patterns(input_df, kmeans=None, scaler=None, fit=True):\n",
    "    \"\"\"Route clustering\"\"\"\n",
    "    route_features = []\n",
    "    \n",
    "    for (gid, pid, nid), group in tqdm(input_df.groupby(['game_id', 'play_id', 'nfl_id']), \n",
    "                                        desc=\"üõ£Ô∏è  Routes\", leave=False):\n",
    "        traj = group.sort_values('frame_id').tail(5)\n",
    "        \n",
    "        if len(traj) < 3:\n",
    "            continue\n",
    "        \n",
    "        positions = traj[['x', 'y']].values\n",
    "        speeds = traj['s'].values\n",
    "        \n",
    "        total_dist = np.sum(np.sqrt(np.diff(positions[:, 0])**2 + np.diff(positions[:, 1])**2))\n",
    "        displacement = np.sqrt((positions[-1, 0] - positions[0, 0])**2 + \n",
    "                              (positions[-1, 1] - positions[0, 1])**2)\n",
    "        straightness = displacement / (total_dist + 0.1)\n",
    "        \n",
    "        angles = np.arctan2(np.diff(positions[:, 1]), np.diff(positions[:, 0]))\n",
    "        if len(angles) > 1:\n",
    "            angle_changes = np.abs(np.diff(angles))\n",
    "            max_turn = np.max(angle_changes)\n",
    "            mean_turn = np.mean(angle_changes)\n",
    "        else:\n",
    "            max_turn = mean_turn = 0\n",
    "        \n",
    "        speed_mean = speeds.mean()\n",
    "        speed_change = speeds[-1] - speeds[0] if len(speeds) > 1 else 0\n",
    "        dx = positions[-1, 0] - positions[0, 0]\n",
    "        dy = positions[-1, 1] - positions[0, 1]\n",
    "        \n",
    "        route_features.append({\n",
    "            'game_id': gid, 'play_id': pid, 'nfl_id': nid,\n",
    "            'traj_straightness': straightness,\n",
    "            'traj_max_turn': max_turn,\n",
    "            'traj_mean_turn': mean_turn,\n",
    "            'traj_depth': abs(dx),\n",
    "            'traj_width': abs(dy),\n",
    "            'speed_mean': speed_mean,\n",
    "            'speed_change': speed_change,\n",
    "        })\n",
    "    \n",
    "    route_df = pd.DataFrame(route_features)\n",
    "    feat_cols = ['traj_straightness', 'traj_max_turn', 'traj_mean_turn',\n",
    "                 'traj_depth', 'traj_width', 'speed_mean', 'speed_change']\n",
    "    X = route_df[feat_cols].fillna(0)\n",
    "    \n",
    "    if fit:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        kmeans = KMeans(n_clusters=Config.N_ROUTE_CLUSTERS, random_state=Config.SEED, n_init=10)\n",
    "        route_df['route_pattern'] = kmeans.fit_predict(X_scaled)\n",
    "        return route_df, kmeans, scaler\n",
    "    else:\n",
    "        X_scaled = scaler.transform(X)\n",
    "        route_df['route_pattern'] = kmeans.predict(X_scaled)\n",
    "        return route_df\n",
    "    \n",
    "def compute_neighbor_embeddings(input_df, k_neigh=Config.K_NEIGH, \n",
    "                                radius=Config.RADIUS, tau=Config.TAU):\n",
    "    \"\"\"GNN-lite embeddings\"\"\"\n",
    "    print(\"üï∏Ô∏è  GNN embeddings...\")\n",
    "    \n",
    "    cols_needed = [\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"x\", \"y\", \n",
    "                   \"velocity_x\", \"velocity_y\", \"player_side\"]\n",
    "    src = input_df[cols_needed].copy()\n",
    "    \n",
    "    last = (src.sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n",
    "               .groupby([\"game_id\", \"play_id\", \"nfl_id\"], as_index=False)\n",
    "               .tail(1)\n",
    "               .rename(columns={\"frame_id\": \"last_frame_id\"})\n",
    "               .reset_index(drop=True))\n",
    "    \n",
    "    tmp = last.merge(\n",
    "        src.rename(columns={\n",
    "            \"frame_id\": \"nb_frame_id\", \"nfl_id\": \"nfl_id_nb\",\n",
    "            \"x\": \"x_nb\", \"y\": \"y_nb\", \n",
    "            \"velocity_x\": \"vx_nb\", \"velocity_y\": \"vy_nb\", \n",
    "            \"player_side\": \"player_side_nb\"\n",
    "        }),\n",
    "        left_on=[\"game_id\", \"play_id\", \"last_frame_id\"],\n",
    "        right_on=[\"game_id\", \"play_id\", \"nb_frame_id\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    tmp = tmp[tmp[\"nfl_id_nb\"] != tmp[\"nfl_id\"]]\n",
    "    tmp[\"dx\"] = tmp[\"x_nb\"] - tmp[\"x\"]\n",
    "    tmp[\"dy\"] = tmp[\"y_nb\"] - tmp[\"y\"]\n",
    "    tmp[\"dvx\"] = tmp[\"vx_nb\"] - tmp[\"velocity_x\"]\n",
    "    tmp[\"dvy\"] = tmp[\"vy_nb\"] - tmp[\"velocity_y\"]\n",
    "    tmp[\"dist\"] = np.sqrt(tmp[\"dx\"]**2 + tmp[\"dy\"]**2)\n",
    "    \n",
    "    tmp = tmp[np.isfinite(tmp[\"dist\"]) & (tmp[\"dist\"] > 1e-6)]\n",
    "    if radius is not None:\n",
    "        tmp = tmp[tmp[\"dist\"] <= radius]\n",
    "    \n",
    "    tmp[\"is_ally\"] = (tmp[\"player_side_nb\"] == tmp[\"player_side\"]).astype(np.float32)\n",
    "    \n",
    "    keys = [\"game_id\", \"play_id\", \"nfl_id\"]\n",
    "    tmp[\"rnk\"] = tmp.groupby(keys)[\"dist\"].rank(method=\"first\")\n",
    "    if k_neigh is not None:\n",
    "        tmp = tmp[tmp[\"rnk\"] <= float(k_neigh)]\n",
    "    \n",
    "    tmp[\"w\"] = np.exp(-tmp[\"dist\"] / float(tau))\n",
    "    sum_w = tmp.groupby(keys)[\"w\"].transform(\"sum\")\n",
    "    tmp[\"wn\"] = np.where(sum_w > 0, tmp[\"w\"] / sum_w, 0.0)\n",
    "    \n",
    "    tmp[\"wn_ally\"] = tmp[\"wn\"] * tmp[\"is_ally\"]\n",
    "    tmp[\"wn_opp\"] = tmp[\"wn\"] * (1.0 - tmp[\"is_ally\"])\n",
    "    \n",
    "    for col in [\"dx\", \"dy\", \"dvx\", \"dvy\"]:\n",
    "        tmp[f\"{col}_ally_w\"] = tmp[col] * tmp[\"wn_ally\"]\n",
    "        tmp[f\"{col}_opp_w\"] = tmp[col] * tmp[\"wn_opp\"]\n",
    "    \n",
    "    tmp[\"dist_ally\"] = np.where(tmp[\"is_ally\"] > 0.5, tmp[\"dist\"], np.nan)\n",
    "    tmp[\"dist_opp\"] = np.where(tmp[\"is_ally\"] < 0.5, tmp[\"dist\"], np.nan)\n",
    "    \n",
    "    ag = tmp.groupby(keys).agg(\n",
    "        gnn_ally_dx_mean=(\"dx_ally_w\", \"sum\"),\n",
    "        gnn_ally_dy_mean=(\"dy_ally_w\", \"sum\"),\n",
    "        gnn_ally_dvx_mean=(\"dvx_ally_w\", \"sum\"),\n",
    "        gnn_ally_dvy_mean=(\"dvy_ally_w\", \"sum\"),\n",
    "        gnn_opp_dx_mean=(\"dx_opp_w\", \"sum\"),\n",
    "        gnn_opp_dy_mean=(\"dy_opp_w\", \"sum\"),\n",
    "        gnn_opp_dvx_mean=(\"dvx_opp_w\", \"sum\"),\n",
    "        gnn_opp_dvy_mean=(\"dvy_opp_w\", \"sum\"),\n",
    "        gnn_ally_cnt=(\"is_ally\", \"sum\"),\n",
    "        gnn_opp_cnt=(\"is_ally\", lambda s: float(len(s) - s.sum())),\n",
    "        gnn_ally_dmin=(\"dist_ally\", \"min\"),\n",
    "        gnn_ally_dmean=(\"dist_ally\", \"mean\"),\n",
    "        gnn_opp_dmin=(\"dist_opp\", \"min\"),\n",
    "        gnn_opp_dmean=(\"dist_opp\", \"mean\"),\n",
    "    ).reset_index()\n",
    "    \n",
    "    near = tmp.loc[tmp[\"rnk\"] <= 3, keys + [\"rnk\", \"dist\"]].copy()\n",
    "    if len(near) > 0:\n",
    "        near[\"rnk\"] = near[\"rnk\"].astype(int)\n",
    "        dwide = near.pivot_table(index=keys, columns=\"rnk\", values=\"dist\", aggfunc=\"first\")\n",
    "        dwide = dwide.rename(columns={1: \"gnn_d1\", 2: \"gnn_d2\", 3: \"gnn_d3\"}).reset_index()\n",
    "        ag = ag.merge(dwide, on=keys, how=\"left\")\n",
    "    \n",
    "    for c in [\"gnn_ally_dx_mean\", \"gnn_ally_dy_mean\", \"gnn_ally_dvx_mean\", \"gnn_ally_dvy_mean\",\n",
    "              \"gnn_opp_dx_mean\", \"gnn_opp_dy_mean\", \"gnn_opp_dvx_mean\", \"gnn_opp_dvy_mean\"]:\n",
    "        ag[c] = ag[c].fillna(0.0)\n",
    "    for c in [\"gnn_ally_cnt\", \"gnn_opp_cnt\"]:\n",
    "        ag[c] = ag[c].fillna(0.0)\n",
    "    for c in [\"gnn_ally_dmin\", \"gnn_opp_dmin\", \"gnn_ally_dmean\", \"gnn_opp_dmean\", \n",
    "              \"gnn_d1\", \"gnn_d2\", \"gnn_d3\"]:\n",
    "        ag[c] = ag[c].fillna(radius if radius is not None else 30.0)\n",
    "    \n",
    "    return ag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b673a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences_geometric(input_df, \n",
    "                                output_df=None, \n",
    "                                test_template=None, \n",
    "                                is_training=True, \n",
    "                                window_size=10,\n",
    "                                route_kmeans=None, \n",
    "                                route_scaler=None):\n",
    "    \"\"\"\n",
    "    YOUR 154 features + 13 geometric features = 167 total\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "        If Training:\n",
    "\n",
    "        If Test:\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PREPARING GEOMETRIC SEQUENCES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    input_df = input_df.copy()\n",
    "    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "    \n",
    "    print(\"Step 1: Base features...\")\n",
    "    \n",
    "    input_df['player_height_feet'] = input_df['player_height'].apply(height_to_feet)\n",
    "    height_parts = input_df['player_height'].str.split('-', expand=True)\n",
    "    input_df['height_inches'] = height_parts[0].astype(float) * 12 + height_parts[1].astype(float)\n",
    "    input_df['bmi'] = (input_df['player_weight'] / (input_df['height_inches']**2)) * 703\n",
    "    \n",
    "    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n",
    "    input_df['velocity_x'] = input_df['s'] * np.sin(dir_rad)\n",
    "    input_df['velocity_y'] = input_df['s'] * np.cos(dir_rad)\n",
    "    # input_df['acceleration_x'] = input_df['a'] * np.cos(dir_rad)\n",
    "    # input_df['acceleration_y'] = input_df['a'] * np.sin(dir_rad)\n",
    "    \n",
    "    input_df['speed_squared'] = input_df['s'] ** 2\n",
    "    # input_df['accel_magnitude'] = np.sqrt(input_df['acceleration_x']**2 + input_df['acceleration_y']**2)\n",
    "    input_df['momentum_x'] = input_df['velocity_x'] * input_df['player_weight']\n",
    "    input_df['momentum_y'] = input_df['velocity_y'] * input_df['player_weight']\n",
    "    input_df['kinetic_energy'] = 0.5 * input_df['player_weight'] * input_df['speed_squared']\n",
    "    \n",
    "    # input_df['orientation_diff'] = np.abs(input_df['o'] - input_df['dir'])\n",
    "    # input_df['orientation_diff'] = np.minimum(input_df['orientation_diff'], 360 - input_df['orientation_diff'])\n",
    "    \n",
    "    input_df['is_offense'] = (input_df['player_side'] == 'Offense').astype(int)\n",
    "    input_df['is_defense'] = (input_df['player_side'] == 'Defense').astype(int)\n",
    "    input_df['is_receiver'] = (input_df['player_role'] == 'Targeted Receiver').astype(int)\n",
    "    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n",
    "    input_df['is_passer'] = (input_df['player_role'] == 'Passer').astype(int)\n",
    "    input_df['role_targeted_receiver'] = input_df['is_receiver']\n",
    "    input_df['role_defensive_coverage'] = input_df['is_coverage']\n",
    "    input_df['role_passer'] = input_df['is_passer']\n",
    "    input_df['side_offense'] = input_df['is_offense']\n",
    "    \n",
    "    if 'ball_land_x' in input_df.columns:\n",
    "        ball_dx = input_df['ball_land_x'] - input_df['x']\n",
    "        ball_dy = input_df['ball_land_y'] - input_df['y']\n",
    "        input_df['distance_to_ball'] = np.sqrt(ball_dx**2 + ball_dy**2)\n",
    "        input_df['dist_to_ball'] = input_df['distance_to_ball']\n",
    "        input_df['dist_squared'] = input_df['distance_to_ball'] ** 2\n",
    "        input_df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n",
    "        input_df['ball_direction_x'] = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n",
    "        input_df['ball_direction_y'] = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n",
    "        input_df['closing_speed_ball'] = (\n",
    "            input_df['velocity_x'] * input_df['ball_direction_x'] +\n",
    "            input_df['velocity_y'] * input_df['ball_direction_y']\n",
    "        )\n",
    "        input_df['velocity_toward_ball'] = (\n",
    "            input_df['velocity_x'] * np.cos(input_df['angle_to_ball']) + \n",
    "            input_df['velocity_y'] * np.sin(input_df['angle_to_ball'])\n",
    "        )\n",
    "        input_df['velocity_alignment'] = np.cos(input_df['angle_to_ball'] - dir_rad)\n",
    "        # input_df['angle_diff'] = np.abs(input_df['o'] - np.degrees(input_df['angle_to_ball']))\n",
    "        # input_df['angle_diff'] = np.minimum(input_df['angle_diff'], 360 - input_df['angle_diff'])\n",
    "    \n",
    "    print(\"Step 2: Advanced features...\")\n",
    "    \n",
    "    opp_features = get_opponent_features(input_df)\n",
    "    input_df = input_df.merge(opp_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n",
    "    \n",
    "    if is_training:\n",
    "        route_features, route_kmeans, route_scaler = extract_route_patterns(input_df)\n",
    "    else:\n",
    "        route_features = extract_route_patterns(input_df, route_kmeans, route_scaler, fit=False)\n",
    "    input_df = input_df.merge(route_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n",
    "    \n",
    "    gnn_features = compute_neighbor_embeddings(input_df)\n",
    "    input_df = input_df.merge(gnn_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n",
    "    \n",
    "    if 'nearest_opp_dist' in input_df.columns:\n",
    "        input_df['pressure'] = 1 / np.maximum(input_df['nearest_opp_dist'], 0.5)\n",
    "        input_df['under_pressure'] = (input_df['nearest_opp_dist'] < 3).astype(int)\n",
    "        input_df['pressure_x_speed'] = input_df['pressure'] * input_df['s']\n",
    "    \n",
    "    if 'mirror_wr_vx' in input_df.columns:\n",
    "        s_safe = np.maximum(input_df['s'], 0.1)\n",
    "        input_df['mirror_similarity'] = (\n",
    "            input_df['velocity_x'] * input_df['mirror_wr_vx'] + \n",
    "            input_df['velocity_y'] * input_df['mirror_wr_vy']\n",
    "        ) / s_safe\n",
    "        input_df['mirror_offset_dist'] = np.sqrt(\n",
    "            input_df['mirror_offset_x']**2 + input_df['mirror_offset_y']**2\n",
    "        )\n",
    "        input_df['mirror_alignment'] = input_df['mirror_similarity'] * input_df['role_defensive_coverage']\n",
    "    \n",
    "    print(\"Step 3: Temporal features...\")\n",
    "    \n",
    "    gcols = ['game_id', 'play_id', 'nfl_id']\n",
    "    \n",
    "    for lag in [1, 2, 3, 4, 5]:\n",
    "        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n",
    "            if col in input_df.columns:\n",
    "                input_df[f'{col}_lag{lag}'] = input_df.groupby(gcols)[col].shift(lag)\n",
    "    \n",
    "    for window in [3, 5]:\n",
    "        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n",
    "            if col in input_df.columns:\n",
    "                input_df[f'{col}_rolling_mean_{window}'] = (\n",
    "                    input_df.groupby(gcols)[col]\n",
    "                      .rolling(window, min_periods=1).mean()\n",
    "                      .reset_index(level=[0,1,2], drop=True)\n",
    "                )\n",
    "                input_df[f'{col}_rolling_std_{window}'] = (\n",
    "                    input_df.groupby(gcols)[col]\n",
    "                      .rolling(window, min_periods=1).std()\n",
    "                      .reset_index(level=[0,1,2], drop=True)\n",
    "                )\n",
    "    \n",
    "    for col in ['velocity_x', 'velocity_y']:\n",
    "        if col in input_df.columns:\n",
    "            input_df[f'{col}_delta'] = input_df.groupby(gcols)[col].diff()\n",
    "    \n",
    "    input_df['velocity_x_ema'] = input_df.groupby(gcols)['velocity_x'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "    input_df['velocity_y_ema'] = input_df.groupby(gcols)['velocity_y'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "    input_df['speed_ema'] = input_df.groupby(gcols)['s'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "    \n",
    "    print(\"Step 4: Time features...\")\n",
    "    \n",
    "    if 'num_frames_output' in input_df.columns:\n",
    "        max_frames = input_df['num_frames_output']\n",
    "        \n",
    "        input_df['max_play_duration'] = max_frames / 10.0\n",
    "        input_df['frame_time'] = input_df['frame_id'] / 10.0\n",
    "        input_df['progress_ratio'] = input_df['frame_id'] / np.maximum(max_frames, 1)\n",
    "        input_df['time_remaining'] = (max_frames - input_df['frame_id']) / 10.0\n",
    "        input_df['frames_remaining'] = max_frames - input_df['frame_id']\n",
    "        \n",
    "        input_df['expected_x_at_ball'] = input_df['x'] + input_df['velocity_x'] * input_df['frame_time']\n",
    "        input_df['expected_y_at_ball'] = input_df['y'] + input_df['velocity_y'] * input_df['frame_time']\n",
    "        \n",
    "        if 'ball_land_x' in input_df.columns:\n",
    "            input_df['error_from_ball_x'] = input_df['expected_x_at_ball'] - input_df['ball_land_x']\n",
    "            input_df['error_from_ball_y'] = input_df['expected_y_at_ball'] - input_df['ball_land_y']\n",
    "            input_df['error_from_ball'] = np.sqrt(\n",
    "                input_df['error_from_ball_x']**2 + input_df['error_from_ball_y']**2\n",
    "            )\n",
    "            \n",
    "            input_df['weighted_dist_by_time'] = input_df['dist_to_ball'] / (input_df['frame_time'] + 0.1)\n",
    "            input_df['dist_scaled_by_progress'] = input_df['dist_to_ball'] * (1 - input_df['progress_ratio'])\n",
    "        \n",
    "        input_df['time_squared'] = input_df['frame_time'] ** 2\n",
    "        input_df['velocity_x_progress'] = input_df['velocity_x'] * input_df['progress_ratio']\n",
    "        input_df['velocity_y_progress'] = input_df['velocity_y'] * input_df['progress_ratio']\n",
    "        input_df['speed_scaled_by_time_left'] = input_df['s'] * input_df['time_remaining']\n",
    "        \n",
    "        input_df['actual_play_length'] = max_frames\n",
    "        input_df['length_ratio'] = max_frames / 30.0\n",
    "    \n",
    "    # üéØ THE BREAKTHROUGH: Add geometric features\n",
    "    print(\"Step 5: üéØ Geometric endpoint features...\")\n",
    "    input_df = add_geometric_features(input_df)\n",
    "    \n",
    "    print(\"Step 6: Building feature list...\")\n",
    "    \n",
    "    # Your 154 proven features\n",
    "    feature_cols = [\n",
    "        'x', 'y', 's', \n",
    "        # 'a', 'o', \n",
    "        'dir', 'frame_id', 'ball_land_x', 'ball_land_y',\n",
    "        'player_height_feet', 'player_weight', 'height_inches', 'bmi',\n",
    "        'velocity_x', 'velocity_y', \n",
    "        # 'acceleration_x', 'acceleration_y',\n",
    "        'momentum_x', 'momentum_y', 'kinetic_energy',\n",
    "        'speed_squared', 'accel_magnitude', \n",
    "        # 'orientation_diff',\n",
    "        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer',\n",
    "        'role_targeted_receiver', 'role_defensive_coverage', 'role_passer', 'side_offense',\n",
    "        'distance_to_ball', 'dist_to_ball', 'dist_squared', 'angle_to_ball', \n",
    "        'ball_direction_x', 'ball_direction_y', 'closing_speed_ball',\n",
    "        'velocity_toward_ball', 'velocity_alignment', \n",
    "        # 'angle_diff',\n",
    "        'nearest_opp_dist', 'closing_speed', 'num_nearby_opp_3', 'num_nearby_opp_5',\n",
    "        'mirror_wr_vx', 'mirror_wr_vy', 'mirror_offset_x', 'mirror_offset_y',\n",
    "        'pressure', 'under_pressure', 'pressure_x_speed', \n",
    "        'mirror_similarity', 'mirror_offset_dist', 'mirror_alignment',\n",
    "        'route_pattern', 'traj_straightness', 'traj_max_turn', 'traj_mean_turn',\n",
    "        'traj_depth', 'traj_width', 'speed_mean', 'speed_change',\n",
    "        'gnn_ally_dx_mean', 'gnn_ally_dy_mean', 'gnn_ally_dvx_mean', 'gnn_ally_dvy_mean',\n",
    "        'gnn_opp_dx_mean', 'gnn_opp_dy_mean', 'gnn_opp_dvx_mean', 'gnn_opp_dvy_mean',\n",
    "        'gnn_ally_cnt', 'gnn_opp_cnt',\n",
    "        'gnn_ally_dmin', 'gnn_ally_dmean', 'gnn_opp_dmin', 'gnn_opp_dmean',\n",
    "        'gnn_d1', 'gnn_d2', 'gnn_d3',\n",
    "    ]\n",
    "    \n",
    "    for lag in [1, 2, 3, 4, 5]:\n",
    "        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n",
    "            feature_cols.append(f'{col}_lag{lag}')\n",
    "    \n",
    "    for window in [3, 5]:\n",
    "        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n",
    "            feature_cols.append(f'{col}_rolling_mean_{window}')\n",
    "            feature_cols.append(f'{col}_rolling_std_{window}')\n",
    "    \n",
    "    feature_cols.extend(['velocity_x_delta', 'velocity_y_delta'])\n",
    "    feature_cols.extend(['velocity_x_ema', 'velocity_y_ema', 'speed_ema'])\n",
    "    \n",
    "    feature_cols.extend([\n",
    "        'max_play_duration', 'frame_time', 'progress_ratio', 'time_remaining', 'frames_remaining',\n",
    "        'expected_x_at_ball', 'expected_y_at_ball', \n",
    "        'error_from_ball_x', 'error_from_ball_y', 'error_from_ball',\n",
    "        'time_squared', 'weighted_dist_by_time', \n",
    "        'velocity_x_progress', 'velocity_y_progress', 'dist_scaled_by_progress',\n",
    "        'speed_scaled_by_time_left', 'actual_play_length', 'length_ratio',\n",
    "    ])\n",
    "    \n",
    "    # üéØ Add 13 geometric features\n",
    "    feature_cols.extend([\n",
    "        'geo_endpoint_x', 'geo_endpoint_y',\n",
    "        'geo_vector_x', 'geo_vector_y', 'geo_distance',\n",
    "        'geo_required_vx', 'geo_required_vy',\n",
    "        'geo_velocity_error_x', 'geo_velocity_error_y', 'geo_velocity_error',\n",
    "        'geo_required_ax', 'geo_required_ay',\n",
    "        'geo_alignment',\n",
    "    ])\n",
    "    \n",
    "    feature_cols = [c for c in feature_cols if c in input_df.columns]\n",
    "    print(f\"‚úì Using {len(feature_cols)} features (154 proven + 13 geometric)\")\n",
    "    \n",
    "    print(\"Step 7: Creating sequences...\")\n",
    "    \n",
    "    target_rows = input_df.copy() # Instantiate before we mess with input_df\n",
    "    target_groups = target_rows[['game_id', 'play_id']].drop_duplicates()\n",
    "\n",
    "    sequences, targets_catch, sequence_ids = [], [], []\n",
    "\n",
    "    for _, row in tqdm(target_groups.iterrows(), total=len(target_groups), desc=\"Creating sequences\"):\n",
    "        # key = (row['game_id'], row['play_id'], row['nfl_id']) \n",
    "        key = (row['game_id'], row['play_id'])\n",
    "        \n",
    "        try:\n",
    "            group_df = input_df[(input_df['game_id']==row['game_id']) &\n",
    "                                 (input_df['play_id']==row['play_id'])]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "        group_df = group_df[group_df['player_role']=='Targeted Receiver']\n",
    "        input_window = group_df.tail(window_size)\n",
    "        \n",
    "        if len(input_window) < window_size:\n",
    "            if is_training:\n",
    "                continue\n",
    "            pad_len = window_size - len(input_window)\n",
    "            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n",
    "            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n",
    "        \n",
    "        input_window = input_window.fillna(group_df.mean(numeric_only=True))\n",
    "        seq = input_window[feature_cols].values\n",
    "        \n",
    "        if np.isnan(seq).any():\n",
    "            if is_training:\n",
    "                continue\n",
    "            seq = np.nan_to_num(seq, nan=0.0)\n",
    "        \n",
    "        sequences.append(seq)\n",
    "        \n",
    "        # Store geometric endpoint for this player\n",
    "        geo_x = input_window.iloc[-1]['geo_endpoint_x']\n",
    "        geo_y = input_window.iloc[-1]['geo_endpoint_y']\n",
    "        \n",
    "        if is_training:\n",
    "            out_grp = output_df[\n",
    "                (output_df['game_id']==group_df.iloc[0]['game_id']) &\n",
    "                (output_df['play_id']==group_df.iloc[0]['play_id']) &\n",
    "                (output_df['nfl_id']==group_df.iloc[0]['nfl_id'])\n",
    "            ].sort_values('frame_id')\n",
    "\n",
    "            was_catch = out_grp['pass_result'].values[0] == 'C'\n",
    "            targets_catch.append(1 if was_catch else 0)\n",
    "            \n",
    "        sequence_ids.append({\n",
    "            'game_id': key[0],\n",
    "            'play_id': key[1],\n",
    "            'frame_id': input_window.iloc[-1]['frame_id']\n",
    "        })\n",
    "\n",
    "    print(f\"‚úì Created {len(sequences)} sequences\")\n",
    "    \n",
    "    if is_training:\n",
    "        return (sequences, \n",
    "                targets_catch,\n",
    "                # targets_dx,\n",
    "                # targets_dy, \n",
    "                # targets_frame_ids, \n",
    "                sequence_ids, \n",
    "                # geo_endpoints_x, \n",
    "                # geo_endpoints_y, \n",
    "                route_kmeans, \n",
    "                route_scaler,\n",
    "                feature_cols)\n",
    "    return sequences, sequence_ids#, geo_endpoints_x, geo_endpoints_y\n",
    "    # return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2cd992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL ARCHITECTURE (YOUR PROVEN GRU + ATTENTION)\n",
    "# ============================================================================\n",
    "class JointSeqModel(nn.Module):\n",
    "    \"\"\"Your proven architecture - unchanged\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
    "        self.pool_ln = nn.LayerNorm(128)\n",
    "        self.pool_attn = nn.MultiheadAttention(128, num_heads=4, batch_first=True)\n",
    "        self.pool_query = nn.Parameter(torch.randn(1, 1, 128))\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(128, 256), \n",
    "            nn.GELU(), \n",
    "            nn.Dropout(0.2), \n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h, _ = self.gru(x)\n",
    "        B = h.size(0)\n",
    "        q = self.pool_query.expand(B, -1, -1)\n",
    "        ctx, _ = self.pool_attn(q, self.pool_ln(h), self.pool_ln(h))\n",
    "        out = self.head(ctx.squeeze(1))\n",
    "        return out.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3d40d7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "# ============================================================================\n",
    "# TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_targets(batch_dx, batch_dy, max_h):\n",
    "    tensors_x, tensors_y, masks = [], [], []\n",
    "    \n",
    "    for dx, dy in zip(batch_dx, batch_dy):\n",
    "        L = len(dx)\n",
    "        padded_x = np.pad(dx, (0, max_h - L), constant_values=0).astype(np.float32)\n",
    "        padded_y = np.pad(dy, (0, max_h - L), constant_values=0).astype(np.float32)\n",
    "        mask = np.zeros(max_h, dtype=np.float32)\n",
    "        mask[:L] = 1.0\n",
    "        \n",
    "        tensors_x.append(torch.tensor(padded_x))\n",
    "        tensors_y.append(torch.tensor(padded_y))\n",
    "        masks.append(torch.tensor(mask))\n",
    "    \n",
    "    targets = torch.stack([torch.stack(tensors_x), torch.stack(tensors_y)], dim=-1)\n",
    "    return targets, torch.stack(masks)\n",
    "\n",
    "def train_model(X_train: List[np.ndarray], \n",
    "                y_train: List[int], \n",
    "                X_val: List[np.ndarray], \n",
    "                y_val: List[int], \n",
    "                input_dim: int, \n",
    "                config: Config):\n",
    "    device = config.DEVICE\n",
    "    model = JointSeqModel(input_dim).to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    \n",
    "    train_batches = []\n",
    "    for i in range(0, len(X_train), config.BATCH_SIZE):\n",
    "        end = min(i + config.BATCH_SIZE, len(X_train))\n",
    "        bx = torch.tensor(np.stack(X_train[i:end]).astype(np.float32))\n",
    "        by = torch.tensor(np.stack(y_train[i:end]).astype(np.float32))\n",
    "        train_batches.append((bx, by))\n",
    "    \n",
    "    val_batches = []\n",
    "    for i in range(0, len(X_val), config.BATCH_SIZE):\n",
    "        end = min(i + config.BATCH_SIZE, len(X_val))\n",
    "        bx = torch.tensor(np.stack(X_val[i:end]).astype(np.float32))\n",
    "        by = torch.tensor(np.stack(y_val[i:end]).astype(np.float32))\n",
    "        val_batches.append((bx, by))\n",
    "    \n",
    "    best_loss, best_state, bad = float('inf'), None, 0\n",
    "    \n",
    "    for epoch in range(1, config.EPOCHS + 1):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for bx, by in train_batches:\n",
    "            bx, by = bx.to(device), by.to(device)\n",
    "            pred = model(bx)\n",
    "            loss = criterion(pred, by)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for bx, by in val_batches:\n",
    "                bx, by = bx.to(device), by.to(device)\n",
    "                pred = model(bx)\n",
    "                val_losses.append(criterion(pred, by).item())\n",
    "                # ADD THESE 2 LINES:\n",
    "                all_preds.append(torch.sigmoid(pred).cpu().numpy())\n",
    "                all_targets.append(by.cpu().numpy())\n",
    "              \n",
    "        train_loss, val_loss = np.mean(train_losses), np.mean(val_losses)\n",
    "        # ADD THESE LINES:\n",
    "        y_pred_proba = np.concatenate(all_preds)\n",
    "        y_true = np.concatenate(all_targets)\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        \n",
    "        auc = roc_auc_score(y_true, y_pred_proba)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"  Epoch {epoch}: train={train_loss:.4f}, val={val_loss:.4f} | \"\n",
    "                f\"AUC={auc:.3f}, Acc={acc:.3f}, Prec={precision:.3f}, Rec={recall:.3f}, F1={f1:.3f}\")\n",
    "      \n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            train_loss_at_best = train_loss\n",
    "            auc_at_best = auc\n",
    "            accuracy_at_best = acc\n",
    "            precision_at_best = precision\n",
    "            recall_at_best = recall\n",
    "            f1_at_best = f1\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= config.PATIENCE:\n",
    "                print(f\"  Early stop at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    return model, best_loss, train_loss_at_best, auc_at_best, accuracy_at_best, precision_at_best, recall_at_best, f1_at_best\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6655dd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PREPARING GEOMETRIC SEQUENCES\n",
      "================================================================================\n",
      "Step 1: Base features...\n",
      "Step 2: Advanced features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üï∏Ô∏è  GNN embeddings...\n",
      "Step 3: Temporal features...\n",
      "Step 4: Time features...\n",
      "Step 5: üéØ Geometric endpoint features...\n",
      "Step 6: Building feature list...\n",
      "‚úì Using 137 features (154 proven + 13 geometric)\n",
      "Step 7: Creating sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating sequences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14108/14108 [01:23<00:00, 168.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created 7403 sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test = train_output.merge(sampled_plays, on=['game_id', 'play_id'], how='left')\n",
    "result = prepare_sequences_geometric(test, output_df = test)\n",
    "sequences,targets_catch, sequence_ids, route_kmeans, route_scale, feature_cols = result\n",
    "\n",
    "sequences = list(sequences)\n",
    "targets_catch = list(targets_catch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4fe91377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_importance(model, X_val, y_val, feature_names, device, n_repeats=3):\n",
    "    \"\"\"Permutation-based feature importance\"\"\"\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    model.eval()\n",
    "    X_val_tensor = torch.tensor(np.stack(X_val).astype(np.float32)).to(device)\n",
    "    y_val_array = np.array(y_val)\n",
    "    \n",
    "    # Baseline score\n",
    "    with torch.no_grad():\n",
    "        baseline_pred = torch.sigmoid(model(X_val_tensor)).cpu().numpy()\n",
    "        baseline_score = roc_auc_score(y_val_array, baseline_pred)\n",
    "    \n",
    "    importances = []\n",
    "    \n",
    "    for feat_idx in tqdm(range(len(feature_names)), desc=\"Computing importances\"):\n",
    "        scores = []\n",
    "        for _ in range(n_repeats):\n",
    "            X_permuted = [x.copy() for x in X_val]\n",
    "            # Permute this feature across all sequences\n",
    "            perm_values = np.random.permutation([x[:, feat_idx] for x in X_val])\n",
    "            for i, x in enumerate(X_permuted):\n",
    "                x[:, feat_idx] = perm_values[i]\n",
    "            \n",
    "            X_perm_tensor = torch.tensor(np.stack(X_permuted).astype(np.float32)).to(device)\n",
    "            with torch.no_grad():\n",
    "                perm_pred = torch.sigmoid(model(X_perm_tensor)).cpu().numpy()\n",
    "                perm_score = roc_auc_score(y_val_array, perm_pred)\n",
    "            scores.append(baseline_score - perm_score)  # Drop in performance\n",
    "        \n",
    "        importances.append(np.mean(scores))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return importance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e078c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/4] Training geometric models...\n",
      "\n",
      "============================================================\n",
      "Fold 1/5\n",
      "============================================================\n",
      "  Epoch 10: train=0.4342, val=0.4253 | AUC=0.873, Acc=0.805, Prec=0.828, Rec=0.861, F1=0.844\n",
      "  Epoch 20: train=0.4029, val=0.4189 | AUC=0.879, Acc=0.822, Prec=0.842, Rec=0.871, F1=0.857\n",
      "  Epoch 30: train=0.3745, val=0.4241 | AUC=0.877, Acc=0.817, Prec=0.833, Rec=0.877, F1=0.854\n",
      "  Epoch 40: train=0.3639, val=0.4262 | AUC=0.877, Acc=0.815, Prec=0.829, Rec=0.879, F1=0.853\n",
      "  Epoch 50: train=0.3609, val=0.4277 | AUC=0.876, Acc=0.811, Prec=0.826, Rec=0.876, F1=0.850\n",
      "  Early stop at epoch 50\n",
      "\n",
      "‚úì Fold 1 validation outcomes - BCE Loss: 0.41893, Train Loss: 0.40287, AUC: 0.879, Acc: 0.822, Prec: 0.842, Rec: 0.871, F1: 0.857\n",
      "üìà Calibration R¬≤ = 0.972\n",
      "\n",
      "üìä Computing feature importance for fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing importances: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 137/137 [00:21<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Fold 2/5\n",
      "============================================================\n",
      "  Epoch 10: train=0.4281, val=0.4536 | AUC=0.857, Acc=0.790, Prec=0.818, Rec=0.843, F1=0.830\n",
      "  Epoch 20: train=0.3934, val=0.4452 | AUC=0.865, Acc=0.798, Prec=0.822, Rec=0.852, F1=0.837\n",
      "  Epoch 30: train=0.3730, val=0.4456 | AUC=0.867, Acc=0.802, Prec=0.820, Rec=0.863, F1=0.841\n",
      "  Epoch 40: train=0.3644, val=0.4504 | AUC=0.867, Acc=0.798, Prec=0.817, Rec=0.861, F1=0.839\n",
      "  Epoch 50: train=0.3602, val=0.4508 | AUC=0.866, Acc=0.799, Prec=0.818, Rec=0.862, F1=0.839\n",
      "  Early stop at epoch 54\n",
      "\n",
      "‚úì Fold 2 validation outcomes - BCE Loss: 0.44324, Train Loss: 0.38214, AUC: 0.867, Acc: 0.800, Prec: 0.819, Rec: 0.862, F1: 0.840\n",
      "üìà Calibration R¬≤ = 0.931\n",
      "\n",
      "üìä Computing feature importance for fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing importances: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 137/137 [00:21<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Fold 3/5\n",
      "============================================================\n",
      "  Epoch 10: train=0.4211, val=0.4410 | AUC=0.866, Acc=0.787, Prec=0.798, Rec=0.854, F1=0.825\n",
      "  Epoch 20: train=0.3859, val=0.4341 | AUC=0.871, Acc=0.797, Prec=0.810, Rec=0.856, F1=0.832\n",
      "  Epoch 30: train=0.3601, val=0.4397 | AUC=0.871, Acc=0.799, Prec=0.815, Rec=0.852, F1=0.833\n",
      "  Epoch 40: train=0.3487, val=0.4435 | AUC=0.871, Acc=0.808, Prec=0.822, Rec=0.860, F1=0.840\n",
      "  Epoch 50: train=0.3496, val=0.4441 | AUC=0.870, Acc=0.808, Prec=0.820, Rec=0.863, F1=0.841\n",
      "  Early stop at epoch 52\n",
      "\n",
      "‚úì Fold 3 validation outcomes - BCE Loss: 0.43227, Train Loss: 0.38242, AUC: 0.872, Acc: 0.796, Prec: 0.808, Rec: 0.859, F1: 0.832\n",
      "üìà Calibration R¬≤ = 0.958\n",
      "\n",
      "üìä Computing feature importance for fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing importances: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 137/137 [00:20<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Fold 4/5\n",
      "============================================================\n",
      "  Epoch 10: train=0.4249, val=0.4304 | AUC=0.868, Acc=0.788, Prec=0.789, Rec=0.879, F1=0.832\n",
      "  Epoch 20: train=0.3910, val=0.4270 | AUC=0.873, Acc=0.795, Prec=0.802, Rec=0.870, F1=0.835\n",
      "  Epoch 30: train=0.3679, val=0.4246 | AUC=0.875, Acc=0.806, Prec=0.807, Rec=0.886, F1=0.845\n",
      "  Epoch 40: train=0.3523, val=0.4279 | AUC=0.875, Acc=0.807, Prec=0.804, Rec=0.894, F1=0.847\n",
      "  Epoch 50: train=0.3455, val=0.4296 | AUC=0.875, Acc=0.808, Prec=0.805, Rec=0.893, F1=0.847\n",
      "  Epoch 60: train=0.3464, val=0.4298 | AUC=0.875, Acc=0.809, Prec=0.806, Rec=0.895, F1=0.848\n",
      "  Early stop at epoch 60\n",
      "\n",
      "‚úì Fold 4 validation outcomes - BCE Loss: 0.42463, Train Loss: 0.36794, AUC: 0.875, Acc: 0.806, Prec: 0.807, Rec: 0.886, F1: 0.845\n",
      "üìà Calibration R¬≤ = 0.980\n",
      "\n",
      "üìä Computing feature importance for fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing importances: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 137/137 [00:23<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Fold 5/5\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[3/4] Training geometric models...\")\n",
    "groups = np.array([d['game_id'] for d in sequence_ids])\n",
    "gkf = GroupKFold(n_splits=config.N_FOLDS)\n",
    "\n",
    "models, scalers = [], []\n",
    "\n",
    "calibration_r2s = []  # ADD THIS BEFORE LOOP\n",
    "importance_dfs = []  # STORE IMPORTANCE DFS\n",
    "\n",
    "for fold, (tr, va) in enumerate(gkf.split(sequences, groups=groups), 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Fold {fold}/{config.N_FOLDS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    X_tr = [sequences[i] for i in tr]\n",
    "    X_va = [sequences[i] for i in va]\n",
    "    y_tr = [targets_catch[i] for i in tr]\n",
    "    y_va = [targets_catch[i] for i in va]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.vstack([s for s in X_tr]))\n",
    "\n",
    "    X_tr_sc = [scaler.transform(s) for s in X_tr]\n",
    "    X_va_sc = [scaler.transform(s) for s in X_va]\n",
    "        \n",
    "    model, loss, train_loss, auc, acc, prec, rec, f1 = train_model(\n",
    "        X_tr_sc, y_tr,\n",
    "        X_va_sc, y_va,\n",
    "        X_tr[0].shape[-1], config = config\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úì Fold {fold} validation outcomes - BCE Loss: {loss:.5f}, Train Loss: {train_loss:.5f}, AUC: {auc:.3f}, Acc: {acc:.3f}, Prec: {prec:.3f}, Rec: {rec:.3f}, F1: {f1:.3f}\")\n",
    "    \n",
    "    # üî• CALIBRATION CHECK\n",
    "    model.eval()\n",
    "    X_va_tensor = torch.tensor(np.stack(X_va_sc).astype(np.float32)).to(config.DEVICE)\n",
    "    with torch.no_grad():\n",
    "        y_va_pred_proba = torch.sigmoid(model(X_va_tensor)).cpu().numpy()\n",
    "    \n",
    "    bins = np.linspace(0, 1, 11)\n",
    "    bin_indices = np.digitize(y_va_pred_proba, bins) - 1\n",
    "    \n",
    "    predicted_probs, actual_rates = [], []\n",
    "    for i in range(10):\n",
    "        mask = (bin_indices == i)\n",
    "        if mask.sum() > 0:\n",
    "            predicted_probs.append(y_va_pred_proba[mask].mean())\n",
    "            actual_rates.append(np.array(y_va)[mask].mean())\n",
    "    \n",
    "    from sklearn.metrics import r2_score\n",
    "    if len(predicted_probs) > 1:\n",
    "        r2 = r2_score(actual_rates, predicted_probs)\n",
    "        calibration_r2s.append(r2)  # üî• STORE IT\n",
    "        print(f\"üìà Calibration R¬≤ = {r2:.3f}\")\n",
    "    \n",
    "    models.append(model)\n",
    "    scalers.append(scaler)\n",
    "     \n",
    "    # Feature importance code...\n",
    "    print(f\"\\nüìä Computing feature importance for fold {fold}...\")\n",
    "    importance_df = compute_feature_importance(\n",
    "        model, X_va_sc, y_va, feature_cols, config.DEVICE, n_repeats=3\n",
    "    )\n",
    "    importance_dfs.append(importance_df)\n",
    "    \n",
    "# üî• FINAL SUMMARY\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL MODEL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Mean Calibration R¬≤: {np.mean(calibration_r2s):.3f} ¬± {np.std(calibration_r2s):.3f}\")\n",
    "print(f\"üèÜ NFL Benchmark: 0.98 | Your Gap: {0.98 - np.mean(calibration_r2s):.3f}\")\n",
    "\n",
    "all_importances = []\n",
    "for fold in range(1, config.N_FOLDS + 1):\n",
    "    df = importance_dfs[fold-1]\n",
    "    all_importances.append(df)\n",
    "\n",
    "avg_importance = pd.concat(all_importances).groupby('feature')['importance'].mean()\n",
    "avg_importance = avg_importance.sort_values(ascending=False).reset_index()\n",
    "\n",
    "print(avg_importance.head(30))\n",
    "avg_importance.to_csv(config.OUTPUT_DIR / 'feature_importance_avg.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2ba71f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Fold 1/5\n",
      "============================================================\n",
      "  Running GridSearchCV...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best params: {'C': 0.01, 'max_iter': 1000}\n",
      "  Best CV score: 0.8549\n",
      "  Validation: BCE=0.4289, AUC=0.8752, Acc=0.807, Prec=0.815, Rec=0.885, F1=0.849\n",
      "\n",
      "üèÜ Top 20 Features:\n",
      "                 feature  importance\n",
      "      geo_velocity_error    0.161527\n",
      "            dist_squared    0.091769\n",
      "        distance_to_ball    0.078398\n",
      "            dist_to_ball    0.078398\n",
      "                pressure    0.054051\n",
      "           geo_alignment    0.052004\n",
      "        velocity_y_delta    0.050838\n",
      "      closing_speed_ball    0.050226\n",
      "    velocity_toward_ball    0.050226\n",
      "velocity_y_rolling_std_3    0.047483\n",
      "           angle_to_ball    0.044558\n",
      "      velocity_alignment    0.038788\n",
      "velocity_x_rolling_std_5    0.038099\n",
      "velocity_x_rolling_std_3    0.036613\n",
      "                     dir    0.031111\n",
      "         s_rolling_std_3    0.030815\n",
      "        velocity_x_delta    0.030257\n",
      "           closing_speed    0.028640\n",
      "        ball_direction_x    0.026713\n",
      "               speed_ema    0.025558\n",
      "\n",
      "============================================================\n",
      "Fold 2/5\n",
      "============================================================\n",
      "  Running GridSearchCV...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best params: {'C': 0.01, 'max_iter': 1000}\n",
      "  Best CV score: 0.8572\n",
      "  Validation: BCE=0.4539, AUC=0.8566, Acc=0.794, Prec=0.811, Rec=0.862, F1=0.836\n",
      "\n",
      "üèÜ Top 20 Features:\n",
      "                 feature  importance\n",
      "      geo_velocity_error    0.152999\n",
      "            dist_squared    0.091729\n",
      "            dist_to_ball    0.080284\n",
      "        distance_to_ball    0.080284\n",
      "           geo_alignment    0.066035\n",
      "        velocity_y_delta    0.058949\n",
      "                pressure    0.053304\n",
      "    velocity_toward_ball    0.051462\n",
      "      closing_speed_ball    0.051445\n",
      "velocity_y_rolling_std_3    0.047797\n",
      "                     dir    0.047758\n",
      "velocity_x_rolling_std_3    0.046267\n",
      "velocity_x_rolling_std_5    0.042611\n",
      "        ball_direction_x    0.033158\n",
      "        velocity_x_delta    0.032192\n",
      "         s_rolling_std_5    0.030883\n",
      "           closing_speed    0.028223\n",
      "         s_rolling_std_3    0.026015\n",
      "               speed_ema    0.023431\n",
      "        ball_direction_y    0.023008\n",
      "\n",
      "============================================================\n",
      "Fold 3/5\n",
      "============================================================\n",
      "  Running GridSearchCV...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best params: {'C': 0.01, 'max_iter': 1000}\n",
      "  Best CV score: 0.8567\n",
      "  Validation: BCE=0.4555, AUC=0.8601, Acc=0.791, Prec=0.798, Rec=0.864, F1=0.830\n",
      "\n",
      "üèÜ Top 20 Features:\n",
      "                 feature  importance\n",
      "      geo_velocity_error    0.129769\n",
      "            dist_squared    0.072823\n",
      "            dist_to_ball    0.064109\n",
      "        distance_to_ball    0.064109\n",
      "           geo_alignment    0.062733\n",
      "                pressure    0.054229\n",
      "    velocity_toward_ball    0.054039\n",
      "      closing_speed_ball    0.054023\n",
      "        velocity_y_delta    0.049656\n",
      "velocity_x_rolling_std_5    0.045430\n",
      "velocity_y_rolling_std_3    0.043827\n",
      "velocity_x_rolling_std_3    0.042451\n",
      "                     dir    0.042138\n",
      "        ball_direction_x    0.036255\n",
      "           angle_to_ball    0.031069\n",
      "        velocity_x_delta    0.030009\n",
      "         s_rolling_std_5    0.027597\n",
      "           closing_speed    0.026854\n",
      "          kinetic_energy    0.023024\n",
      "         y_rolling_std_5    0.022890\n",
      "\n",
      "============================================================\n",
      "Fold 4/5\n",
      "============================================================\n",
      "  Running GridSearchCV...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best params: {'C': 0.01, 'max_iter': 1000}\n",
      "  Best CV score: 0.8571\n",
      "  Validation: BCE=0.4328, AUC=0.8682, Acc=0.799, Prec=0.800, Rec=0.884, F1=0.840\n",
      "\n",
      "üèÜ Top 20 Features:\n",
      "                 feature  importance\n",
      "      geo_velocity_error    0.154190\n",
      "            dist_squared    0.089915\n",
      "            dist_to_ball    0.081308\n",
      "        distance_to_ball    0.081308\n",
      "           geo_alignment    0.064723\n",
      "    velocity_toward_ball    0.054884\n",
      "      closing_speed_ball    0.054867\n",
      "velocity_y_rolling_std_3    0.054152\n",
      "                pressure    0.053361\n",
      "                     dir    0.047603\n",
      "        velocity_y_delta    0.047120\n",
      "velocity_x_rolling_std_3    0.042131\n",
      "velocity_x_rolling_std_5    0.037840\n",
      "         s_rolling_std_5    0.035507\n",
      "        ball_direction_x    0.033255\n",
      "        velocity_x_delta    0.029533\n",
      "           closing_speed    0.027499\n",
      "           angle_to_ball    0.026777\n",
      "               speed_ema    0.026022\n",
      "velocity_y_rolling_std_5    0.025001\n",
      "\n",
      "============================================================\n",
      "Fold 5/5\n",
      "============================================================\n",
      "  Running GridSearchCV...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/kniu91/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best params: {'C': 0.01, 'max_iter': 1000}\n",
      "  Best CV score: 0.8641\n",
      "  Validation: BCE=0.4714, AUC=0.8473, Acc=0.786, Prec=0.777, Rec=0.880, F1=0.825\n",
      "\n",
      "üèÜ Top 20 Features:\n",
      "                 feature  importance\n",
      "      geo_velocity_error    0.139883\n",
      "            dist_squared    0.088948\n",
      "            dist_to_ball    0.085317\n",
      "        distance_to_ball    0.085317\n",
      "           geo_alignment    0.067200\n",
      "                     dir    0.053841\n",
      "    velocity_toward_ball    0.053504\n",
      "      closing_speed_ball    0.053486\n",
      "                pressure    0.051957\n",
      "velocity_y_rolling_std_3    0.048394\n",
      "        velocity_y_delta    0.047085\n",
      "velocity_x_rolling_std_5    0.044914\n",
      "        ball_direction_x    0.039706\n",
      "           angle_to_ball    0.039562\n",
      "velocity_x_rolling_std_3    0.032820\n",
      "        velocity_x_delta    0.031347\n",
      "         s_rolling_std_3    0.030433\n",
      "         s_rolling_std_5    0.029248\n",
      "           closing_speed    0.028073\n",
      "        pressure_x_speed    0.022307\n",
      "\n",
      "================================================================================\n",
      "AGGREGATED FEATURE IMPORTANCE ACROSS FOLDS\n",
      "================================================================================\n",
      "                     feature  importance\n",
      "0         geo_velocity_error    0.147673\n",
      "1               dist_squared    0.087037\n",
      "2               dist_to_ball    0.077883\n",
      "3           distance_to_ball    0.077883\n",
      "4              geo_alignment    0.062539\n",
      "5                   pressure    0.053380\n",
      "6       velocity_toward_ball    0.052823\n",
      "7         closing_speed_ball    0.052809\n",
      "8           velocity_y_delta    0.050730\n",
      "9   velocity_y_rolling_std_3    0.048330\n",
      "10                       dir    0.044490\n",
      "11  velocity_x_rolling_std_5    0.041779\n",
      "12  velocity_x_rolling_std_3    0.040057\n",
      "13          ball_direction_x    0.033817\n",
      "14             angle_to_ball    0.032460\n",
      "15          velocity_x_delta    0.030668\n",
      "16           s_rolling_std_5    0.028976\n",
      "17             closing_speed    0.027858\n",
      "18           s_rolling_std_3    0.026556\n",
      "19        velocity_alignment    0.023617\n",
      "20                 speed_ema    0.022667\n",
      "21  velocity_y_rolling_std_5    0.021557\n",
      "22          pressure_x_speed    0.021158\n",
      "23              geo_distance    0.020958\n",
      "24          num_nearby_opp_3    0.020151\n",
      "25            kinetic_energy    0.019942\n",
      "26           x_rolling_std_5    0.019027\n",
      "27             speed_squared    0.018886\n",
      "28          ball_direction_y    0.018823\n",
      "29          s_rolling_mean_5    0.017531\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "for fold, (tr, va) in enumerate(gkf.split(sequences, groups=groups), 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Fold {fold}/{config.N_FOLDS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    X_tr = [sequences[i] for i in tr]\n",
    "    X_va = [sequences[i] for i in va]\n",
    "    y_tr = [targets_catch[i] for i in tr]\n",
    "    y_va = [targets_catch[i] for i in va]\n",
    "\n",
    "    # Flatten sequences: (num_samples, 10 timesteps, 153 features) -> (num_samples, 1530)\n",
    "    X_tr_flat = np.vstack([s.flatten() for s in X_tr])\n",
    "    X_va_flat = np.vstack([s.flatten() for s in X_va])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_tr_sc = scaler.fit_transform(X_tr_flat)\n",
    "    X_va_sc = scaler.transform(X_va_flat)\n",
    "    \n",
    "    # Grid search for best C (inverse regularization strength)\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],  # L2 penalty strength\n",
    "        'max_iter': [1000]\n",
    "    }\n",
    "    \n",
    "    base_model = LogisticRegression(penalty='l2', solver='lbfgs', random_state=config.SEED)\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        base_model, \n",
    "        param_grid, \n",
    "        cv=3,  # 3-fold CV within training data\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(f\"  Running GridSearchCV...\")\n",
    "    grid_search.fit(X_tr_sc, y_tr)\n",
    "    \n",
    "    print(f\"  Best params: {grid_search.best_params_}\")\n",
    "    print(f\"  Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Use best model\n",
    "    model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    y_va_pred_proba = model.predict_proba(X_va_sc)[:, 1]\n",
    "    y_va_pred = (y_va_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # üî• ADD BCE COMPUTATION\n",
    "    bce_loss = log_loss(y_va, y_va_pred_proba)\n",
    "    \n",
    "    val_auc = roc_auc_score(y_va, y_va_pred_proba)\n",
    "    acc = accuracy_score(y_va, y_va_pred)\n",
    "    precision = precision_score(y_va, y_va_pred, zero_division=0)\n",
    "    recall = recall_score(y_va, y_va_pred, zero_division=0)\n",
    "    f1 = f1_score(y_va, y_va_pred, zero_division=0)\n",
    "    \n",
    "    # üî• UPDATE PRINT TO INCLUDE BCE\n",
    "    print(f\"  Validation: BCE={bce_loss:.4f}, AUC={val_auc:.4f}, Acc={acc:.3f}, \"\n",
    "          f\"Prec={precision:.3f}, Rec={recall:.3f}, F1={f1:.3f}\")\n",
    "    \n",
    "    models.append(model)\n",
    "    scalers.append(scaler)\n",
    "    \n",
    "    # Feature importance from coefficients\n",
    "    coef = model.coef_[0]  # Shape: (1530,) for flattened features\n",
    "    \n",
    "    # Aggregate across timesteps: (10 timesteps, 153 features) -> (153,)\n",
    "    coef_reshaped = coef.reshape(10, len(feature_cols))\n",
    "    feature_importance = np.abs(coef_reshaped).mean(axis=0)  # Average across time\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nüèÜ Top 20 Features:\")\n",
    "    print(importance_df.head(20).to_string(index=False))\n",
    "    \n",
    "    importance_df.to_csv(config.OUTPUT_DIR / f'importance_fold{fold}_logreg.csv', index=False)\n",
    "\n",
    "# After all folds, aggregate\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AGGREGATED FEATURE IMPORTANCE ACROSS FOLDS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_importances = []\n",
    "for fold in range(1, config.N_FOLDS + 1):\n",
    "    df = pd.read_csv(config.OUTPUT_DIR / f'importance_fold{fold}_logreg.csv')\n",
    "    all_importances.append(df)\n",
    "\n",
    "avg_importance = pd.concat(all_importances).groupby('feature')['importance'].mean()\n",
    "avg_importance = avg_importance.sort_values(ascending=False).reset_index()\n",
    "print(avg_importance.head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c000d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5cf42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON, config\n",
    "    )\n",
    "    \n",
    "    models.append(model)\n",
    "    scalers.append(scaler)\n",
    "    \n",
    "    print(f\"\\n‚úì Fold {fold} - Loss: {loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ee877",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for fold, (tr, va) in enumerate(gkf.split(sequences, groups=groups), 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Fold {fold}/{config.N_FOLDS}\")\n",
    "    print(f\"{'='*60}\")a\n",
    "    \n",
    "    X_tr = [sequences[i] for i in tr]\n",
    "    X_va = [sequences[i] for i in va]\n",
    "    y_tr_dx = [targets_dx[i] for i in tr]\n",
    "    y_va_dx = [targets_dx[i] for i in va]\n",
    "    y_tr_dy = [targets_dy[i] for i in tr]\n",
    "    y_va_dy = [targets_dy[i] for i in va]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.vstack([s for s in X_tr]))\n",
    "    \n",
    "    X_tr_sc = [scaler.transform(s) for s in X_tr]\n",
    "    X_va_sc = [scaler.transform(s) for s in X_va]\n",
    "    \n",
    "    model, loss = train_model(\n",
    "        X_tr_sc, y_tr_dx, y_tr_dy,\n",
    "        X_va_sc, y_va_dx, y_va_dy,\n",
    "        X_tr[0].shape[-1], config.MAX_FUTURE_HORIZON, config\n",
    "    )\n",
    "    \n",
    "    models.append(model)\n",
    "    scalers.append(scaler)\n",
    "    \n",
    "    print(f\"\\n‚úì Fold {fold} - Loss: {loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85efdec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c09bc508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PREPARING GEOMETRIC SEQUENCES\n",
      "================================================================================\n",
      "Step 1: Base features...\n",
      "Step 2: Advanced features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     ]\r"
     ]
    }
   ],
   "source": [
    "a = prepare_sequences_geometric(train_input, train_output, is_training=True, window_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "355ad55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_id</th>\n",
       "      <th>play_id</th>\n",
       "      <th>player_to_predict</th>\n",
       "      <th>nfl_id</th>\n",
       "      <th>frame_id</th>\n",
       "      <th>play_direction</th>\n",
       "      <th>absolute_yardline_number</th>\n",
       "      <th>player_name</th>\n",
       "      <th>player_height</th>\n",
       "      <th>player_weight</th>\n",
       "      <th>player_birth_date</th>\n",
       "      <th>player_position</th>\n",
       "      <th>player_side</th>\n",
       "      <th>player_role</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>s</th>\n",
       "      <th>a</th>\n",
       "      <th>dir</th>\n",
       "      <th>o</th>\n",
       "      <th>num_frames_output</th>\n",
       "      <th>ball_land_x</th>\n",
       "      <th>ball_land_y</th>\n",
       "      <th>player_height_feet</th>\n",
       "      <th>height_inches</th>\n",
       "      <th>bmi</th>\n",
       "      <th>velocity_x</th>\n",
       "      <th>velocity_y</th>\n",
       "      <th>acceleration_x</th>\n",
       "      <th>acceleration_y</th>\n",
       "      <th>speed_squared</th>\n",
       "      <th>accel_magnitude</th>\n",
       "      <th>momentum_x</th>\n",
       "      <th>momentum_y</th>\n",
       "      <th>kinetic_energy</th>\n",
       "      <th>orientation_diff</th>\n",
       "      <th>is_offense</th>\n",
       "      <th>is_defense</th>\n",
       "      <th>is_receiver</th>\n",
       "      <th>is_coverage</th>\n",
       "      <th>is_passer</th>\n",
       "      <th>role_targeted_receiver</th>\n",
       "      <th>role_defensive_coverage</th>\n",
       "      <th>role_passer</th>\n",
       "      <th>side_offense</th>\n",
       "      <th>distance_to_ball</th>\n",
       "      <th>dist_to_ball</th>\n",
       "      <th>dist_squared</th>\n",
       "      <th>angle_to_ball</th>\n",
       "      <th>ball_direction_x</th>\n",
       "      <th>ball_direction_y</th>\n",
       "      <th>closing_speed_ball</th>\n",
       "      <th>velocity_toward_ball</th>\n",
       "      <th>velocity_alignment</th>\n",
       "      <th>angle_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>2023090700</td>\n",
       "      <td>101</td>\n",
       "      <td>False</td>\n",
       "      <td>43290</td>\n",
       "      <td>1</td>\n",
       "      <td>right</td>\n",
       "      <td>42</td>\n",
       "      <td>Jared Goff</td>\n",
       "      <td>6-4</td>\n",
       "      <td>223</td>\n",
       "      <td>1994-10-14</td>\n",
       "      <td>QB</td>\n",
       "      <td>Offense</td>\n",
       "      <td>Passer</td>\n",
       "      <td>37.36</td>\n",
       "      <td>30.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>65.42</td>\n",
       "      <td>95.98</td>\n",
       "      <td>21</td>\n",
       "      <td>63.259998</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>76.0</td>\n",
       "      <td>27.141447</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>30.56</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>39.853407</td>\n",
       "      <td>39.853407</td>\n",
       "      <td>1588.294013</td>\n",
       "      <td>-0.863368</td>\n",
       "      <td>0.649882</td>\n",
       "      <td>-0.760035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.420835</td>\n",
       "      <td>145.447319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>2023090700</td>\n",
       "      <td>101</td>\n",
       "      <td>False</td>\n",
       "      <td>43290</td>\n",
       "      <td>2</td>\n",
       "      <td>right</td>\n",
       "      <td>42</td>\n",
       "      <td>Jared Goff</td>\n",
       "      <td>6-4</td>\n",
       "      <td>223</td>\n",
       "      <td>1994-10-14</td>\n",
       "      <td>QB</td>\n",
       "      <td>Offense</td>\n",
       "      <td>Passer</td>\n",
       "      <td>37.36</td>\n",
       "      <td>30.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>63.91</td>\n",
       "      <td>95.98</td>\n",
       "      <td>21</td>\n",
       "      <td>63.259998</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>76.0</td>\n",
       "      <td>27.141447</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>32.07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>39.853407</td>\n",
       "      <td>39.853407</td>\n",
       "      <td>1588.294013</td>\n",
       "      <td>-0.863368</td>\n",
       "      <td>0.649882</td>\n",
       "      <td>-0.760035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.396785</td>\n",
       "      <td>145.447319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>2023090700</td>\n",
       "      <td>101</td>\n",
       "      <td>False</td>\n",
       "      <td>43290</td>\n",
       "      <td>3</td>\n",
       "      <td>right</td>\n",
       "      <td>42</td>\n",
       "      <td>Jared Goff</td>\n",
       "      <td>6-4</td>\n",
       "      <td>223</td>\n",
       "      <td>1994-10-14</td>\n",
       "      <td>QB</td>\n",
       "      <td>Offense</td>\n",
       "      <td>Passer</td>\n",
       "      <td>37.35</td>\n",
       "      <td>30.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>53.83</td>\n",
       "      <td>95.98</td>\n",
       "      <td>21</td>\n",
       "      <td>63.259998</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>76.0</td>\n",
       "      <td>27.141447</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>42.15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>39.859906</td>\n",
       "      <td>39.859906</td>\n",
       "      <td>1588.812113</td>\n",
       "      <td>-0.863177</td>\n",
       "      <td>0.650027</td>\n",
       "      <td>-0.759911</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.229819</td>\n",
       "      <td>145.436394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>2023090700</td>\n",
       "      <td>101</td>\n",
       "      <td>False</td>\n",
       "      <td>43290</td>\n",
       "      <td>4</td>\n",
       "      <td>right</td>\n",
       "      <td>42</td>\n",
       "      <td>Jared Goff</td>\n",
       "      <td>6-4</td>\n",
       "      <td>223</td>\n",
       "      <td>1994-10-14</td>\n",
       "      <td>QB</td>\n",
       "      <td>Offense</td>\n",
       "      <td>Passer</td>\n",
       "      <td>37.34</td>\n",
       "      <td>30.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>310.79</td>\n",
       "      <td>95.98</td>\n",
       "      <td>21</td>\n",
       "      <td>63.259998</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>76.0</td>\n",
       "      <td>27.141447</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>145.19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>39.866407</td>\n",
       "      <td>39.866407</td>\n",
       "      <td>1589.330413</td>\n",
       "      <td>-0.862986</td>\n",
       "      <td>0.650171</td>\n",
       "      <td>-0.759788</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>145.425473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>2023090700</td>\n",
       "      <td>101</td>\n",
       "      <td>False</td>\n",
       "      <td>43290</td>\n",
       "      <td>5</td>\n",
       "      <td>right</td>\n",
       "      <td>42</td>\n",
       "      <td>Jared Goff</td>\n",
       "      <td>6-4</td>\n",
       "      <td>223</td>\n",
       "      <td>1994-10-14</td>\n",
       "      <td>QB</td>\n",
       "      <td>Offense</td>\n",
       "      <td>Passer</td>\n",
       "      <td>37.33</td>\n",
       "      <td>30.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.37</td>\n",
       "      <td>271.81</td>\n",
       "      <td>96.80</td>\n",
       "      <td>21</td>\n",
       "      <td>63.259998</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>76.0</td>\n",
       "      <td>27.141447</td>\n",
       "      <td>-0.05997</td>\n",
       "      <td>0.001895</td>\n",
       "      <td>0.043272</td>\n",
       "      <td>-1.369316</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>1.37</td>\n",
       "      <td>-13.373324</td>\n",
       "      <td>0.42261</td>\n",
       "      <td>0.4014</td>\n",
       "      <td>175.01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>39.872910</td>\n",
       "      <td>39.872910</td>\n",
       "      <td>1589.848913</td>\n",
       "      <td>-0.862796</td>\n",
       "      <td>0.650316</td>\n",
       "      <td>-0.759664</td>\n",
       "      <td>-0.040439</td>\n",
       "      <td>-0.040439</td>\n",
       "      <td>0.779825</td>\n",
       "      <td>146.234555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        game_id  play_id  player_to_predict  nfl_id  frame_id play_direction  \\\n",
       "182  2023090700      101              False   43290         1          right   \n",
       "183  2023090700      101              False   43290         2          right   \n",
       "184  2023090700      101              False   43290         3          right   \n",
       "185  2023090700      101              False   43290         4          right   \n",
       "186  2023090700      101              False   43290         5          right   \n",
       "\n",
       "     absolute_yardline_number player_name player_height  player_weight  \\\n",
       "182                        42  Jared Goff           6-4            223   \n",
       "183                        42  Jared Goff           6-4            223   \n",
       "184                        42  Jared Goff           6-4            223   \n",
       "185                        42  Jared Goff           6-4            223   \n",
       "186                        42  Jared Goff           6-4            223   \n",
       "\n",
       "    player_birth_date player_position player_side player_role      x      y  \\\n",
       "182        1994-10-14              QB     Offense      Passer  37.36  30.07   \n",
       "183        1994-10-14              QB     Offense      Passer  37.36  30.07   \n",
       "184        1994-10-14              QB     Offense      Passer  37.35  30.07   \n",
       "185        1994-10-14              QB     Offense      Passer  37.34  30.07   \n",
       "186        1994-10-14              QB     Offense      Passer  37.33  30.07   \n",
       "\n",
       "        s     a     dir      o  num_frames_output  ball_land_x  ball_land_y  \\\n",
       "182  0.00  0.00   65.42  95.98                 21    63.259998        -0.22   \n",
       "183  0.00  0.00   63.91  95.98                 21    63.259998        -0.22   \n",
       "184  0.00  0.00   53.83  95.98                 21    63.259998        -0.22   \n",
       "185  0.00  0.00  310.79  95.98                 21    63.259998        -0.22   \n",
       "186  0.06  1.37  271.81  96.80                 21    63.259998        -0.22   \n",
       "\n",
       "     player_height_feet  height_inches        bmi  velocity_x  velocity_y  \\\n",
       "182            6.333333           76.0  27.141447     0.00000    0.000000   \n",
       "183            6.333333           76.0  27.141447     0.00000    0.000000   \n",
       "184            6.333333           76.0  27.141447     0.00000    0.000000   \n",
       "185            6.333333           76.0  27.141447    -0.00000    0.000000   \n",
       "186            6.333333           76.0  27.141447    -0.05997    0.001895   \n",
       "\n",
       "     acceleration_x  acceleration_y  speed_squared  accel_magnitude  \\\n",
       "182        0.000000        0.000000         0.0000             0.00   \n",
       "183        0.000000        0.000000         0.0000             0.00   \n",
       "184        0.000000        0.000000         0.0000             0.00   \n",
       "185        0.000000       -0.000000         0.0000             0.00   \n",
       "186        0.043272       -1.369316         0.0036             1.37   \n",
       "\n",
       "     momentum_x  momentum_y  kinetic_energy  orientation_diff  is_offense  \\\n",
       "182    0.000000     0.00000          0.0000             30.56           1   \n",
       "183    0.000000     0.00000          0.0000             32.07           1   \n",
       "184    0.000000     0.00000          0.0000             42.15           1   \n",
       "185   -0.000000     0.00000          0.0000            145.19           1   \n",
       "186  -13.373324     0.42261          0.4014            175.01           1   \n",
       "\n",
       "     is_defense  is_receiver  is_coverage  is_passer  role_targeted_receiver  \\\n",
       "182           0            0            0          1                       0   \n",
       "183           0            0            0          1                       0   \n",
       "184           0            0            0          1                       0   \n",
       "185           0            0            0          1                       0   \n",
       "186           0            0            0          1                       0   \n",
       "\n",
       "     role_defensive_coverage  role_passer  side_offense  distance_to_ball  \\\n",
       "182                        0            1             1         39.853407   \n",
       "183                        0            1             1         39.853407   \n",
       "184                        0            1             1         39.859906   \n",
       "185                        0            1             1         39.866407   \n",
       "186                        0            1             1         39.872910   \n",
       "\n",
       "     dist_to_ball  dist_squared  angle_to_ball  ball_direction_x  \\\n",
       "182     39.853407   1588.294013      -0.863368          0.649882   \n",
       "183     39.853407   1588.294013      -0.863368          0.649882   \n",
       "184     39.859906   1588.812113      -0.863177          0.650027   \n",
       "185     39.866407   1589.330413      -0.862986          0.650171   \n",
       "186     39.872910   1589.848913      -0.862796          0.650316   \n",
       "\n",
       "     ball_direction_y  closing_speed_ball  velocity_toward_ball  \\\n",
       "182         -0.760035            0.000000              0.000000   \n",
       "183         -0.760035            0.000000              0.000000   \n",
       "184         -0.759911            0.000000              0.000000   \n",
       "185         -0.759788           -0.000000             -0.000000   \n",
       "186         -0.759664           -0.040439             -0.040439   \n",
       "\n",
       "     velocity_alignment  angle_diff  \n",
       "182           -0.420835  145.447319  \n",
       "183           -0.396785  145.447319  \n",
       "184           -0.229819  145.436394  \n",
       "185            0.999992  145.425473  \n",
       "186            0.779825  146.234555  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd919b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Get Opponent Features\n",
    "# ==========================================\n",
    "# This includes mirror / wr tracking\n",
    "# Seems like wecan include because it only has speed and positions?\n",
    "\n",
    "# TODO: Implement opponent features\n",
    "# Hopefully we just need to update some variable names after getting kinematics in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9e5b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Extract route patterns\n",
    "# ==========================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f05a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Compute neighbor embeddings\n",
    "# ==========================================\n",
    "# This is the GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20f497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Create temporal features (rolling mean)\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e85b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Time features\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804ec7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Geometric features\n",
    "# =========================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
