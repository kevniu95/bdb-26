{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-10T04:17:38.333441Z",
     "iopub.status.busy": "2025-12-10T04:17:38.332934Z",
     "iopub.status.idle": "2025-12-10T04:19:13.126667Z",
     "shell.execute_reply": "2025-12-10T04:19:13.126014Z",
     "shell.execute_reply.started": "2025-12-10T04:17:38.333418Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†\n",
      "ğŸ†                                                                              ğŸ†\n",
      "ğŸ†  NFL BIG DATA BOWL 2026 - GEOMETRIC NEURAL BREAKTHROUGH     ğŸ†\n",
      "ğŸ†  Proven NN (0.59) + Geometric Insights (Our Discovery)      ğŸ†\n",
      "ğŸ†                                                                              ğŸ†\n",
      "ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†ğŸ†\n",
      "\n",
      "ğŸ¯ TARGET: 0.54-0.56 LB\n",
      "\n",
      "\n",
      "[1/4] Loading data...\n",
      "âœ“ Train input: (4880579, 23), Train output: (562936, 6)\n",
      "âœ“ Test input: (49753, 23), Test template: (5837, 5)\n",
      "\n",
      "[2/4] Preparing geometric sequences...\n",
      "\n",
      "================================================================================\n",
      "PREPARING GEOMETRIC SEQUENCES\n",
      "================================================================================\n",
      "Step 1: Base features...\n",
      "Step 2: Advanced features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸˆ Opponents:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ›£ï¸  Routes:   0%|          | 0/6150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ•¸ï¸  GNN embeddings...\n",
      "Step 3: Temporal features...\n",
      "Step 4: Time features...\n",
      "Step 5: ğŸ¯ Geometric endpoint features...\n",
      "Step 6: Building feature list...\n",
      "âœ“ Using 167 features (154 proven + 13 geometric)\n",
      "Step 7: Creating sequences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d29b53635b4abba9eac2ece4f4daec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating sequences:   0%|          | 0/1611 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 1610 sequences\n",
      "\n",
      "[3/4] Training geometric models...\n",
      "\n",
      "============================================================\n",
      "Fold 1/5\n",
      "============================================================\n",
      "  Epoch 10: train=0.2364, val=0.3287 | RMSE=1.2148\n",
      "  Epoch 20: train=0.1560, val=0.2607 | RMSE=1.1259\n",
      "  Epoch 30: train=0.1001, val=0.2373 | RMSE=1.0872\n",
      "  Epoch 40: train=0.0992, val=0.2147 | RMSE=1.0181\n",
      "  Epoch 50: train=0.0806, val=0.2112 | RMSE=1.0230\n",
      "  Epoch 60: train=0.0710, val=0.2120 | RMSE=1.0237\n",
      "  Epoch 70: train=0.0757, val=0.2096 | RMSE=1.0124\n",
      "  Epoch 80: train=0.0730, val=0.2090 | RMSE=1.0149\n",
      "  Epoch 90: train=0.0700, val=0.2095 | RMSE=1.0155\n",
      "  Early stop at epoch 95\n",
      "\n",
      "âœ“ Fold 1 - Huber Loss: 0.20785 | RMSE: 1.0113\n",
      "\n",
      "============================================================\n",
      "Fold 2/5\n",
      "============================================================\n",
      "  Epoch 10: train=0.1989, val=0.2408 | RMSE=1.0176\n",
      "  Epoch 20: train=0.1417, val=0.2396 | RMSE=1.0351\n",
      "  Epoch 30: train=0.0987, val=0.1939 | RMSE=0.9137\n",
      "  Epoch 40: train=0.0872, val=0.1915 | RMSE=0.9181\n",
      "  Epoch 50: train=0.0765, val=0.1885 | RMSE=0.8995\n",
      "  Epoch 60: train=0.0734, val=0.1880 | RMSE=0.9038\n",
      "  Epoch 70: train=0.0719, val=0.1874 | RMSE=0.9030\n",
      "  Epoch 80: train=0.0701, val=0.1876 | RMSE=0.9033\n",
      "  Early stop at epoch 84\n",
      "\n",
      "âœ“ Fold 2 - Huber Loss: 0.18540 | RMSE: 0.8990\n",
      "\n",
      "============================================================\n",
      "Fold 3/5\n",
      "============================================================\n",
      "  Epoch 10: train=0.2039, val=0.2119 | RMSE=1.1426\n",
      "  Epoch 20: train=0.1283, val=0.1840 | RMSE=1.0199\n",
      "  Epoch 30: train=0.1149, val=0.1810 | RMSE=1.0357\n",
      "  Epoch 40: train=0.0950, val=0.1694 | RMSE=1.0540\n",
      "  Epoch 50: train=0.0689, val=0.1422 | RMSE=0.9854\n",
      "  Epoch 60: train=0.0645, val=0.1432 | RMSE=0.9947\n",
      "  Epoch 70: train=0.0609, val=0.1391 | RMSE=0.9732\n",
      "  Early stop at epoch 79\n",
      "\n",
      "âœ“ Fold 3 - Huber Loss: 0.13409 | RMSE: 0.9248\n",
      "\n",
      "============================================================\n",
      "Fold 4/5\n",
      "============================================================\n",
      "  Epoch 10: train=0.2098, val=0.2773 | RMSE=1.0978\n",
      "  Epoch 20: train=0.1785, val=0.2414 | RMSE=1.0006\n",
      "  Epoch 30: train=0.1257, val=0.1922 | RMSE=0.8887\n",
      "  Epoch 40: train=0.0890, val=0.1667 | RMSE=0.8320\n",
      "  Epoch 50: train=0.0812, val=0.1660 | RMSE=0.8206\n",
      "  Epoch 60: train=0.0828, val=0.1655 | RMSE=0.8167\n",
      "  Epoch 70: train=0.0762, val=0.1624 | RMSE=0.8154\n",
      "  Epoch 80: train=0.0697, val=0.1584 | RMSE=0.8030\n",
      "  Epoch 90: train=0.0708, val=0.1607 | RMSE=0.8158\n",
      "  Early stop at epoch 92\n",
      "\n",
      "âœ“ Fold 4 - Huber Loss: 0.15631 | RMSE: 0.7915\n",
      "\n",
      "============================================================\n",
      "Fold 5/5\n",
      "============================================================\n",
      "  Epoch 10: train=0.2707, val=0.2309 | RMSE=0.8352\n",
      "  Epoch 20: train=0.1967, val=0.2025 | RMSE=0.7691\n",
      "  Epoch 30: train=0.1457, val=0.1523 | RMSE=0.6408\n",
      "  Epoch 40: train=0.0993, val=0.1160 | RMSE=0.5546\n",
      "  Epoch 50: train=0.0863, val=0.1164 | RMSE=0.5519\n",
      "  Epoch 60: train=0.0773, val=0.1122 | RMSE=0.5433\n",
      "  Epoch 70: train=0.0722, val=0.1136 | RMSE=0.5418\n",
      "  Epoch 80: train=0.0675, val=0.1116 | RMSE=0.5419\n",
      "  Epoch 90: train=0.0714, val=0.1121 | RMSE=0.5416\n",
      "  Early stop at epoch 93\n",
      "\n",
      "âœ“ Fold 5 - Huber Loss: 0.11131 | RMSE: 0.5413\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š CROSS-VALIDATION SUMMARY\n",
      "============================================================\n",
      "Mean RMSE: 0.8336 Â± 0.1621\n",
      "Best RMSE: 0.5413\n",
      "Worst RMSE: 1.0113\n",
      "============================================================\n",
      "\n",
      "\n",
      "ğŸ’¾ Exporting models and artifacts...\n",
      "  âœ“ Saved fold 1: model_fold1.pth, scaler_fold1.pkl\n",
      "  âœ“ Saved fold 2: model_fold2.pth, scaler_fold2.pkl\n",
      "  âœ“ Saved fold 3: model_fold3.pth, scaler_fold3.pkl\n",
      "  âœ“ Saved fold 4: model_fold4.pth, scaler_fold4.pkl\n",
      "  âœ“ Saved fold 5: model_fold5.pth, scaler_fold5.pkl\n",
      "  âœ“ Saved route_kmeans.pkl, route_scaler.pkl, metadata.pkl\n",
      "\n",
      "ğŸ‰ All artifacts exported to: outputs/trained_models\n",
      "   Download this folder to run inference locally!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NFL BIG DATA BOWL 2026 - GEOMETRIC NEURAL BREAKTHROUGH\n",
    "ğŸ† The Winning Architecture\n",
    "\n",
    "INSIGHT: Football players follow geometric rules with learned corrections\n",
    "- Receivers â†’ Ball landing (geometric)\n",
    "- Defenders â†’ Mirror receivers (geometric coupling)  \n",
    "- Others â†’ Momentum (physics)\n",
    "- Model learns only CORRECTIONS for coverage, collisions, boundaries\n",
    "\n",
    "Architecture:\n",
    "âœ“ Proven GRU + Attention (your 0.59 base)\n",
    "âœ“ 154 proven features (unchanged)\n",
    "âœ“ +15 geometric features (our discovery)\n",
    "âœ“ Train on corrections to geometric baseline (the breakthrough)\n",
    "\n",
    "Target: 0.54-0.56 LB\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.cluster import KMeans\n",
    "from multiprocessing import Pool as MultiprocessingPool, cpu_count\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n",
    "    OUTPUT_DIR = Path(\"./outputs\")\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    SEED = 42\n",
    "    N_FOLDS = 5\n",
    "    BATCH_SIZE = 256\n",
    "    EPOCHS = 200\n",
    "    PATIENCE = 30\n",
    "    LEARNING_RATE = 1e-3\n",
    "    \n",
    "    WINDOW_SIZE = 10\n",
    "    HIDDEN_DIM = 128\n",
    "    MAX_FUTURE_HORIZON = 94\n",
    "    \n",
    "    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n",
    "    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n",
    "    \n",
    "    K_NEIGH = 6\n",
    "    RADIUS = 30.0\n",
    "    TAU = 8.0\n",
    "    N_ROUTE_CLUSTERS = 7\n",
    "    \n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(Config.SEED)\n",
    "\n",
    "# ============================================================================\n",
    "# GEOMETRIC BASELINE - THE BREAKTHROUGH\n",
    "# ============================================================================\n",
    "\n",
    "def compute_geometric_endpoint(df):\n",
    "    \"\"\"\n",
    "    Compute where each player SHOULD end up based on geometry.\n",
    "    This is the deterministic part - no learning needed.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Time to play end\n",
    "    if 'num_frames_output' in df.columns:\n",
    "        t_total = df['num_frames_output'] / 10.0\n",
    "    else:\n",
    "        t_total = 3.0\n",
    "    \n",
    "    df['time_to_endpoint'] = t_total\n",
    "    \n",
    "    # Initialize with momentum (default rule)\n",
    "    df['geo_endpoint_x'] = df['x'] + df['velocity_x'] * t_total\n",
    "    df['geo_endpoint_y'] = df['y'] + df['velocity_y'] * t_total\n",
    "    \n",
    "    # Rule 1: Targeted Receivers converge to ball\n",
    "    if 'ball_land_x' in df.columns:\n",
    "        receiver_mask = df['player_role'] == 'Targeted Receiver'\n",
    "        df.loc[receiver_mask, 'geo_endpoint_x'] = df.loc[receiver_mask, 'ball_land_x']\n",
    "        df.loc[receiver_mask, 'geo_endpoint_y'] = df.loc[receiver_mask, 'ball_land_y']\n",
    "        \n",
    "        # Rule 2: Defenders mirror receivers (maintain offset)\n",
    "        defender_mask = df['player_role'] == 'Defensive Coverage'\n",
    "        has_mirror = df.get('mirror_offset_x', 0).notna() & (df.get('mirror_wr_dist', 50) < 15)\n",
    "        coverage_mask = defender_mask & has_mirror\n",
    "        \n",
    "        df.loc[coverage_mask, 'geo_endpoint_x'] = (\n",
    "            df.loc[coverage_mask, 'ball_land_x'] + \n",
    "            df.loc[coverage_mask, 'mirror_offset_x'].fillna(0)\n",
    "        )\n",
    "        df.loc[coverage_mask, 'geo_endpoint_y'] = (\n",
    "            df.loc[coverage_mask, 'ball_land_y'] + \n",
    "            df.loc[coverage_mask, 'mirror_offset_y'].fillna(0)\n",
    "        )\n",
    "    \n",
    "    # Clip to field\n",
    "    df['geo_endpoint_x'] = df['geo_endpoint_x'].clip(Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n",
    "    df['geo_endpoint_y'] = df['geo_endpoint_y'].clip(Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_geometric_features(df):\n",
    "    \"\"\"Add features that describe the geometric solution\"\"\"\n",
    "    df = compute_geometric_endpoint(df)\n",
    "    \n",
    "    # Vector to geometric endpoint\n",
    "    df['geo_vector_x'] = df['geo_endpoint_x'] - df['x']\n",
    "    df['geo_vector_y'] = df['geo_endpoint_y'] - df['y']\n",
    "    df['geo_distance'] = np.sqrt(df['geo_vector_x']**2 + df['geo_vector_y']**2)\n",
    "    \n",
    "    # Required velocity to reach geometric endpoint\n",
    "    t = df['time_to_endpoint'] + 0.1\n",
    "    df['geo_required_vx'] = df['geo_vector_x'] / t\n",
    "    df['geo_required_vy'] = df['geo_vector_y'] / t\n",
    "    \n",
    "    # Current velocity vs required\n",
    "    df['geo_velocity_error_x'] = df['geo_required_vx'] - df['velocity_x']\n",
    "    df['geo_velocity_error_y'] = df['geo_required_vy'] - df['velocity_y']\n",
    "    df['geo_velocity_error'] = np.sqrt(\n",
    "        df['geo_velocity_error_x']**2 + df['geo_velocity_error_y']**2\n",
    "    )\n",
    "    \n",
    "    # Required constant acceleration (a = 2*Î”x/tÂ²)\n",
    "    t_sq = t * t\n",
    "    df['geo_required_ax'] = 2 * df['geo_vector_x'] / t_sq\n",
    "    df['geo_required_ay'] = 2 * df['geo_vector_y'] / t_sq\n",
    "    df['geo_required_ax'] = df['geo_required_ax'].clip(-10, 10)\n",
    "    df['geo_required_ay'] = df['geo_required_ay'].clip(-10, 10)\n",
    "    \n",
    "    # Alignment with geometric path\n",
    "    velocity_mag = np.sqrt(df['velocity_x']**2 + df['velocity_y']**2)\n",
    "    geo_unit_x = df['geo_vector_x'] / (df['geo_distance'] + 0.1)\n",
    "    geo_unit_y = df['geo_vector_y'] / (df['geo_distance'] + 0.1)\n",
    "    df['geo_alignment'] = (\n",
    "        df['velocity_x'] * geo_unit_x + df['velocity_y'] * geo_unit_y\n",
    "    ) / (velocity_mag + 0.1)\n",
    "    \n",
    "    # Role-specific geometric quality\n",
    "    df['geo_receiver_urgency'] = df['is_receiver'] * df['geo_distance'] / (t + 0.1)\n",
    "    df['geo_defender_coupling'] = df['is_coverage'] * (1.0 / (df.get('mirror_wr_dist', 50) + 1.0))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# PROVEN FEATURE ENGINEERING (YOUR 0.59 BASE)\n",
    "# ============================================================================\n",
    "\n",
    "def get_velocity(speed, direction_deg):\n",
    "    theta = np.deg2rad(direction_deg)\n",
    "    return speed * np.sin(theta), speed * np.cos(theta)\n",
    "\n",
    "def height_to_feet(height_str):\n",
    "    try:\n",
    "        ft, inches = map(int, str(height_str).split('-'))\n",
    "        return ft + inches/12\n",
    "    except:\n",
    "        return 6.0\n",
    "\n",
    "def get_opponent_features(input_df):\n",
    "    \"\"\"Enhanced opponent interaction with MIRROR WR tracking\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for (gid, pid), group in tqdm(input_df.groupby(['game_id', 'play_id']), \n",
    "                                   desc=\"ğŸˆ Opponents\", leave=False):\n",
    "        last = group.sort_values('frame_id').groupby('nfl_id').last()\n",
    "        \n",
    "        if len(last) < 2:\n",
    "            continue\n",
    "            \n",
    "        positions = last[['x', 'y']].values\n",
    "        sides = last['player_side'].values\n",
    "        speeds = last['s'].values\n",
    "        directions = last['dir'].values\n",
    "        roles = last['player_role'].values\n",
    "        \n",
    "        receiver_mask = np.isin(roles, ['Targeted Receiver', 'Other Route Runner'])\n",
    "        \n",
    "        for i, (nid, side, role) in enumerate(zip(last.index, sides, roles)):\n",
    "            opp_mask = sides != side\n",
    "            \n",
    "            feat = {\n",
    "                'game_id': gid, 'play_id': pid, 'nfl_id': nid,\n",
    "                'nearest_opp_dist': 50.0, 'closing_speed': 0.0,\n",
    "                'num_nearby_opp_3': 0, 'num_nearby_opp_5': 0,\n",
    "                'mirror_wr_vx': 0.0, 'mirror_wr_vy': 0.0,\n",
    "                'mirror_offset_x': 0.0, 'mirror_offset_y': 0.0,\n",
    "                'mirror_wr_dist': 50.0,\n",
    "            }\n",
    "            \n",
    "            if not opp_mask.any():\n",
    "                features.append(feat)\n",
    "                continue\n",
    "            \n",
    "            opp_positions = positions[opp_mask]\n",
    "            distances = np.sqrt(((positions[i] - opp_positions)**2).sum(axis=1))\n",
    "            \n",
    "            if len(distances) == 0:\n",
    "                features.append(feat)\n",
    "                continue\n",
    "                \n",
    "            nearest_idx = distances.argmin()\n",
    "            feat['nearest_opp_dist'] = distances[nearest_idx]\n",
    "            feat['num_nearby_opp_3'] = (distances < 3.0).sum()\n",
    "            feat['num_nearby_opp_5'] = (distances < 5.0).sum()\n",
    "            \n",
    "            my_vx, my_vy = get_velocity(speeds[i], directions[i])\n",
    "            opp_speeds = speeds[opp_mask]\n",
    "            opp_dirs = directions[opp_mask]\n",
    "            opp_vx, opp_vy = get_velocity(opp_speeds[nearest_idx], opp_dirs[nearest_idx])\n",
    "            \n",
    "            rel_vx = my_vx - opp_vx\n",
    "            rel_vy = my_vy - opp_vy\n",
    "            to_me = positions[i] - opp_positions[nearest_idx]\n",
    "            to_me_norm = to_me / (np.linalg.norm(to_me) + 0.1)\n",
    "            feat['closing_speed'] = -(rel_vx * to_me_norm[0] + rel_vy * to_me_norm[1])\n",
    "            \n",
    "            if role == 'Defensive Coverage' and receiver_mask.any():\n",
    "                rec_positions = positions[receiver_mask]\n",
    "                rec_distances = np.sqrt(((positions[i] - rec_positions)**2).sum(axis=1))\n",
    "                \n",
    "                if len(rec_distances) > 0:\n",
    "                    closest_rec_idx = rec_distances.argmin()\n",
    "                    rec_indices = np.where(receiver_mask)[0]\n",
    "                    actual_rec_idx = rec_indices[closest_rec_idx]\n",
    "                    \n",
    "                    rec_vx, rec_vy = get_velocity(speeds[actual_rec_idx], directions[actual_rec_idx])\n",
    "                    \n",
    "                    feat['mirror_wr_vx'] = rec_vx\n",
    "                    feat['mirror_wr_vy'] = rec_vy\n",
    "                    feat['mirror_wr_dist'] = rec_distances[closest_rec_idx]\n",
    "                    feat['mirror_offset_x'] = positions[i][0] - rec_positions[closest_rec_idx][0]\n",
    "                    feat['mirror_offset_y'] = positions[i][1] - rec_positions[closest_rec_idx][1]\n",
    "            \n",
    "            features.append(feat)\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "def extract_route_patterns(input_df, kmeans=None, scaler=None, fit=True):\n",
    "    \"\"\"Route clustering\"\"\"\n",
    "    route_features = []\n",
    "    \n",
    "    for (gid, pid, nid), group in tqdm(input_df.groupby(['game_id', 'play_id', 'nfl_id']), \n",
    "                                        desc=\"ğŸ›£ï¸  Routes\", leave=False):\n",
    "        traj = group.sort_values('frame_id').tail(5)\n",
    "        \n",
    "        if len(traj) < 3:\n",
    "            continue\n",
    "        \n",
    "        positions = traj[['x', 'y']].values\n",
    "        speeds = traj['s'].values\n",
    "        \n",
    "        total_dist = np.sum(np.sqrt(np.diff(positions[:, 0])**2 + np.diff(positions[:, 1])**2))\n",
    "        displacement = np.sqrt((positions[-1, 0] - positions[0, 0])**2 + \n",
    "                              (positions[-1, 1] - positions[0, 1])**2)\n",
    "        straightness = displacement / (total_dist + 0.1)\n",
    "        \n",
    "        angles = np.arctan2(np.diff(positions[:, 1]), np.diff(positions[:, 0]))\n",
    "        if len(angles) > 1:\n",
    "            angle_changes = np.abs(np.diff(angles))\n",
    "            max_turn = np.max(angle_changes)\n",
    "            mean_turn = np.mean(angle_changes)\n",
    "        else:\n",
    "            max_turn = mean_turn = 0\n",
    "        \n",
    "        speed_mean = speeds.mean()\n",
    "        speed_change = speeds[-1] - speeds[0] if len(speeds) > 1 else 0\n",
    "        dx = positions[-1, 0] - positions[0, 0]\n",
    "        dy = positions[-1, 1] - positions[0, 1]\n",
    "        \n",
    "        route_features.append({\n",
    "            'game_id': gid, 'play_id': pid, 'nfl_id': nid,\n",
    "            'traj_straightness': straightness,\n",
    "            'traj_max_turn': max_turn,\n",
    "            'traj_mean_turn': mean_turn,\n",
    "            'traj_depth': abs(dx),\n",
    "            'traj_width': abs(dy),\n",
    "            'speed_mean': speed_mean,\n",
    "            'speed_change': speed_change,\n",
    "        })\n",
    "    \n",
    "    route_df = pd.DataFrame(route_features)\n",
    "    feat_cols = ['traj_straightness', 'traj_max_turn', 'traj_mean_turn',\n",
    "                 'traj_depth', 'traj_width', 'speed_mean', 'speed_change']\n",
    "    X = route_df[feat_cols].fillna(0)\n",
    "    \n",
    "    if fit:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        kmeans = KMeans(n_clusters=Config.N_ROUTE_CLUSTERS, random_state=Config.SEED, n_init=10)\n",
    "        route_df['route_pattern'] = kmeans.fit_predict(X_scaled)\n",
    "        return route_df, kmeans, scaler\n",
    "    else:\n",
    "        X_scaled = scaler.transform(X)\n",
    "        route_df['route_pattern'] = kmeans.predict(X_scaled)\n",
    "        return route_df\n",
    "\n",
    "def compute_neighbor_embeddings(input_df, k_neigh=Config.K_NEIGH, \n",
    "                                radius=Config.RADIUS, tau=Config.TAU):\n",
    "    \"\"\"GNN-lite embeddings\"\"\"\n",
    "    print(\"ğŸ•¸ï¸  GNN embeddings...\")\n",
    "    \n",
    "    cols_needed = [\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"x\", \"y\", \n",
    "                   \"velocity_x\", \"velocity_y\", \"player_side\"]\n",
    "    src = input_df[cols_needed].copy()\n",
    "    \n",
    "    last = (src.sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n",
    "               .groupby([\"game_id\", \"play_id\", \"nfl_id\"], as_index=False)\n",
    "               .tail(1)\n",
    "               .rename(columns={\"frame_id\": \"last_frame_id\"})\n",
    "               .reset_index(drop=True))\n",
    "    \n",
    "    tmp = last.merge(\n",
    "        src.rename(columns={\n",
    "            \"frame_id\": \"nb_frame_id\", \"nfl_id\": \"nfl_id_nb\",\n",
    "            \"x\": \"x_nb\", \"y\": \"y_nb\", \n",
    "            \"velocity_x\": \"vx_nb\", \"velocity_y\": \"vy_nb\", \n",
    "            \"player_side\": \"player_side_nb\"\n",
    "        }),\n",
    "        left_on=[\"game_id\", \"play_id\", \"last_frame_id\"],\n",
    "        right_on=[\"game_id\", \"play_id\", \"nb_frame_id\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    tmp = tmp[tmp[\"nfl_id_nb\"] != tmp[\"nfl_id\"]]\n",
    "    tmp[\"dx\"] = tmp[\"x_nb\"] - tmp[\"x\"]\n",
    "    tmp[\"dy\"] = tmp[\"y_nb\"] - tmp[\"y\"]\n",
    "    tmp[\"dvx\"] = tmp[\"vx_nb\"] - tmp[\"velocity_x\"]\n",
    "    tmp[\"dvy\"] = tmp[\"vy_nb\"] - tmp[\"velocity_y\"]\n",
    "    tmp[\"dist\"] = np.sqrt(tmp[\"dx\"]**2 + tmp[\"dy\"]**2)\n",
    "    \n",
    "    tmp = tmp[np.isfinite(tmp[\"dist\"]) & (tmp[\"dist\"] > 1e-6)]\n",
    "    if radius is not None:\n",
    "        tmp = tmp[tmp[\"dist\"] <= radius]\n",
    "    \n",
    "    tmp[\"is_ally\"] = (tmp[\"player_side_nb\"] == tmp[\"player_side\"]).astype(np.float32)\n",
    "    \n",
    "    keys = [\"game_id\", \"play_id\", \"nfl_id\"]\n",
    "    tmp[\"rnk\"] = tmp.groupby(keys)[\"dist\"].rank(method=\"first\")\n",
    "    if k_neigh is not None:\n",
    "        tmp = tmp[tmp[\"rnk\"] <= float(k_neigh)]\n",
    "    \n",
    "    tmp[\"w\"] = np.exp(-tmp[\"dist\"] / float(tau))\n",
    "    sum_w = tmp.groupby(keys)[\"w\"].transform(\"sum\")\n",
    "    tmp[\"wn\"] = np.where(sum_w > 0, tmp[\"w\"] / sum_w, 0.0)\n",
    "    \n",
    "    tmp[\"wn_ally\"] = tmp[\"wn\"] * tmp[\"is_ally\"]\n",
    "    tmp[\"wn_opp\"] = tmp[\"wn\"] * (1.0 - tmp[\"is_ally\"])\n",
    "    \n",
    "    for col in [\"dx\", \"dy\", \"dvx\", \"dvy\"]:\n",
    "        tmp[f\"{col}_ally_w\"] = tmp[col] * tmp[\"wn_ally\"]\n",
    "        tmp[f\"{col}_opp_w\"] = tmp[col] * tmp[\"wn_opp\"]\n",
    "    \n",
    "    tmp[\"dist_ally\"] = np.where(tmp[\"is_ally\"] > 0.5, tmp[\"dist\"], np.nan)\n",
    "    tmp[\"dist_opp\"] = np.where(tmp[\"is_ally\"] < 0.5, tmp[\"dist\"], np.nan)\n",
    "    \n",
    "    ag = tmp.groupby(keys).agg(\n",
    "        gnn_ally_dx_mean=(\"dx_ally_w\", \"sum\"),\n",
    "        gnn_ally_dy_mean=(\"dy_ally_w\", \"sum\"),\n",
    "        gnn_ally_dvx_mean=(\"dvx_ally_w\", \"sum\"),\n",
    "        gnn_ally_dvy_mean=(\"dvy_ally_w\", \"sum\"),\n",
    "        gnn_opp_dx_mean=(\"dx_opp_w\", \"sum\"),\n",
    "        gnn_opp_dy_mean=(\"dy_opp_w\", \"sum\"),\n",
    "        gnn_opp_dvx_mean=(\"dvx_opp_w\", \"sum\"),\n",
    "        gnn_opp_dvy_mean=(\"dvy_opp_w\", \"sum\"),\n",
    "        gnn_ally_cnt=(\"is_ally\", \"sum\"),\n",
    "        gnn_opp_cnt=(\"is_ally\", lambda s: float(len(s) - s.sum())),\n",
    "        gnn_ally_dmin=(\"dist_ally\", \"min\"),\n",
    "        gnn_ally_dmean=(\"dist_ally\", \"mean\"),\n",
    "        gnn_opp_dmin=(\"dist_opp\", \"min\"),\n",
    "        gnn_opp_dmean=(\"dist_opp\", \"mean\"),\n",
    "    ).reset_index()\n",
    "    \n",
    "    near = tmp.loc[tmp[\"rnk\"] <= 3, keys + [\"rnk\", \"dist\"]].copy()\n",
    "    if len(near) > 0:\n",
    "        near[\"rnk\"] = near[\"rnk\"].astype(int)\n",
    "        dwide = near.pivot_table(index=keys, columns=\"rnk\", values=\"dist\", aggfunc=\"first\")\n",
    "        dwide = dwide.rename(columns={1: \"gnn_d1\", 2: \"gnn_d2\", 3: \"gnn_d3\"}).reset_index()\n",
    "        ag = ag.merge(dwide, on=keys, how=\"left\")\n",
    "    \n",
    "    for c in [\"gnn_ally_dx_mean\", \"gnn_ally_dy_mean\", \"gnn_ally_dvx_mean\", \"gnn_ally_dvy_mean\",\n",
    "              \"gnn_opp_dx_mean\", \"gnn_opp_dy_mean\", \"gnn_opp_dvx_mean\", \"gnn_opp_dvy_mean\"]:\n",
    "        ag[c] = ag[c].fillna(0.0)\n",
    "    for c in [\"gnn_ally_cnt\", \"gnn_opp_cnt\"]:\n",
    "        ag[c] = ag[c].fillna(0.0)\n",
    "    for c in [\"gnn_ally_dmin\", \"gnn_opp_dmin\", \"gnn_ally_dmean\", \"gnn_opp_dmean\", \n",
    "              \"gnn_d1\", \"gnn_d2\", \"gnn_d3\"]:\n",
    "        ag[c] = ag[c].fillna(radius if radius is not None else 30.0)\n",
    "    \n",
    "    return ag\n",
    "\n",
    "# ============================================================================\n",
    "# SEQUENCE PREPARATION WITH GEOMETRIC FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_sequences_geometric(input_df, output_df=None, test_template=None, \n",
    "                                is_training=True, window_size=10,\n",
    "                                route_kmeans=None, route_scaler=None):\n",
    "    \"\"\"YOUR 154 features + 13 geometric features = 167 total\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PREPARING GEOMETRIC SEQUENCES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    input_df = input_df.copy()\n",
    "    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "    \n",
    "    print(\"Step 1: Base features...\")\n",
    "    \n",
    "    input_df['player_height_feet'] = input_df['player_height'].apply(height_to_feet)\n",
    "    height_parts = input_df['player_height'].str.split('-', expand=True)\n",
    "    input_df['height_inches'] = height_parts[0].astype(float) * 12 + height_parts[1].astype(float)\n",
    "    input_df['bmi'] = (input_df['player_weight'] / (input_df['height_inches']**2)) * 703\n",
    "    \n",
    "    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n",
    "    input_df['velocity_x'] = input_df['s'] * np.sin(dir_rad)\n",
    "    input_df['velocity_y'] = input_df['s'] * np.cos(dir_rad)\n",
    "    input_df['acceleration_x'] = input_df['a'] * np.cos(dir_rad)\n",
    "    input_df['acceleration_y'] = input_df['a'] * np.sin(dir_rad)\n",
    "    \n",
    "    input_df['speed_squared'] = input_df['s'] ** 2\n",
    "    input_df['accel_magnitude'] = np.sqrt(input_df['acceleration_x']**2 + input_df['acceleration_y']**2)\n",
    "    input_df['momentum_x'] = input_df['velocity_x'] * input_df['player_weight']\n",
    "    input_df['momentum_y'] = input_df['velocity_y'] * input_df['player_weight']\n",
    "    input_df['kinetic_energy'] = 0.5 * input_df['player_weight'] * input_df['speed_squared']\n",
    "    \n",
    "    input_df['orientation_diff'] = np.abs(input_df['o'] - input_df['dir'])\n",
    "    input_df['orientation_diff'] = np.minimum(input_df['orientation_diff'], 360 - input_df['orientation_diff'])\n",
    "    \n",
    "    input_df['is_offense'] = (input_df['player_side'] == 'Offense').astype(int)\n",
    "    input_df['is_defense'] = (input_df['player_side'] == 'Defense').astype(int)\n",
    "    input_df['is_receiver'] = (input_df['player_role'] == 'Targeted Receiver').astype(int)\n",
    "    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n",
    "    input_df['is_passer'] = (input_df['player_role'] == 'Passer').astype(int)\n",
    "    input_df['role_targeted_receiver'] = input_df['is_receiver']\n",
    "    input_df['role_defensive_coverage'] = input_df['is_coverage']\n",
    "    input_df['role_passer'] = input_df['is_passer']\n",
    "    input_df['side_offense'] = input_df['is_offense']\n",
    "    \n",
    "    if 'ball_land_x' in input_df.columns:\n",
    "        ball_dx = input_df['ball_land_x'] - input_df['x']\n",
    "        ball_dy = input_df['ball_land_y'] - input_df['y']\n",
    "        input_df['distance_to_ball'] = np.sqrt(ball_dx**2 + ball_dy**2)\n",
    "        input_df['dist_to_ball'] = input_df['distance_to_ball']\n",
    "        input_df['dist_squared'] = input_df['distance_to_ball'] ** 2\n",
    "        input_df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n",
    "        input_df['ball_direction_x'] = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n",
    "        input_df['ball_direction_y'] = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n",
    "        input_df['closing_speed_ball'] = (\n",
    "            input_df['velocity_x'] * input_df['ball_direction_x'] +\n",
    "            input_df['velocity_y'] * input_df['ball_direction_y']\n",
    "        )\n",
    "        input_df['velocity_toward_ball'] = (\n",
    "            input_df['velocity_x'] * np.cos(input_df['angle_to_ball']) + \n",
    "            input_df['velocity_y'] * np.sin(input_df['angle_to_ball'])\n",
    "        )\n",
    "        input_df['velocity_alignment'] = np.cos(input_df['angle_to_ball'] - dir_rad)\n",
    "        input_df['angle_diff'] = np.abs(input_df['o'] - np.degrees(input_df['angle_to_ball']))\n",
    "        input_df['angle_diff'] = np.minimum(input_df['angle_diff'], 360 - input_df['angle_diff'])\n",
    "    \n",
    "    print(\"Step 2: Advanced features...\")\n",
    "    \n",
    "    opp_features = get_opponent_features(input_df)\n",
    "    input_df = input_df.merge(opp_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n",
    "    \n",
    "    if is_training:\n",
    "        route_features, route_kmeans, route_scaler = extract_route_patterns(input_df)\n",
    "    else:\n",
    "        route_features = extract_route_patterns(input_df, route_kmeans, route_scaler, fit=False)\n",
    "    input_df = input_df.merge(route_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n",
    "    \n",
    "    gnn_features = compute_neighbor_embeddings(input_df)\n",
    "    input_df = input_df.merge(gnn_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n",
    "    \n",
    "    if 'nearest_opp_dist' in input_df.columns:\n",
    "        input_df['pressure'] = 1 / np.maximum(input_df['nearest_opp_dist'], 0.5)\n",
    "        input_df['under_pressure'] = (input_df['nearest_opp_dist'] < 3).astype(int)\n",
    "        input_df['pressure_x_speed'] = input_df['pressure'] * input_df['s']\n",
    "    \n",
    "    if 'mirror_wr_vx' in input_df.columns:\n",
    "        s_safe = np.maximum(input_df['s'], 0.1)\n",
    "        input_df['mirror_similarity'] = (\n",
    "            input_df['velocity_x'] * input_df['mirror_wr_vx'] + \n",
    "            input_df['velocity_y'] * input_df['mirror_wr_vy']\n",
    "        ) / s_safe\n",
    "        input_df['mirror_offset_dist'] = np.sqrt(\n",
    "            input_df['mirror_offset_x']**2 + input_df['mirror_offset_y']**2\n",
    "        )\n",
    "        input_df['mirror_alignment'] = input_df['mirror_similarity'] * input_df['role_defensive_coverage']\n",
    "    \n",
    "    print(\"Step 3: Temporal features...\")\n",
    "    \n",
    "    gcols = ['game_id', 'play_id', 'nfl_id']\n",
    "    \n",
    "    for lag in [1, 2, 3, 4, 5]:\n",
    "        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n",
    "            if col in input_df.columns:\n",
    "                input_df[f'{col}_lag{lag}'] = input_df.groupby(gcols)[col].shift(lag)\n",
    "    \n",
    "    for window in [3, 5]:\n",
    "        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n",
    "            if col in input_df.columns:\n",
    "                input_df[f'{col}_rolling_mean_{window}'] = (\n",
    "                    input_df.groupby(gcols)[col]\n",
    "                      .rolling(window, min_periods=1).mean()\n",
    "                      .reset_index(level=[0,1,2], drop=True)\n",
    "                )\n",
    "                input_df[f'{col}_rolling_std_{window}'] = (\n",
    "                    input_df.groupby(gcols)[col]\n",
    "                      .rolling(window, min_periods=1).std()\n",
    "                      .reset_index(level=[0,1,2], drop=True)\n",
    "                )\n",
    "    \n",
    "    for col in ['velocity_x', 'velocity_y']:\n",
    "        if col in input_df.columns:\n",
    "            input_df[f'{col}_delta'] = input_df.groupby(gcols)[col].diff()\n",
    "    \n",
    "    input_df['velocity_x_ema'] = input_df.groupby(gcols)['velocity_x'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "    input_df['velocity_y_ema'] = input_df.groupby(gcols)['velocity_y'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "    input_df['speed_ema'] = input_df.groupby(gcols)['s'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "    \n",
    "    print(\"Step 4: Time features...\")\n",
    "    \n",
    "    if 'num_frames_output' in input_df.columns:\n",
    "        max_frames = input_df['num_frames_output']\n",
    "        \n",
    "        input_df['max_play_duration'] = max_frames / 10.0\n",
    "        input_df['frame_time'] = input_df['frame_id'] / 10.0\n",
    "        input_df['progress_ratio'] = input_df['frame_id'] / np.maximum(max_frames, 1)\n",
    "        input_df['time_remaining'] = (max_frames - input_df['frame_id']) / 10.0\n",
    "        input_df['frames_remaining'] = max_frames - input_df['frame_id']\n",
    "        \n",
    "        input_df['expected_x_at_ball'] = input_df['x'] + input_df['velocity_x'] * input_df['frame_time']\n",
    "        input_df['expected_y_at_ball'] = input_df['y'] + input_df['velocity_y'] * input_df['frame_time']\n",
    "        \n",
    "        if 'ball_land_x' in input_df.columns:\n",
    "            input_df['error_from_ball_x'] = input_df['expected_x_at_ball'] - input_df['ball_land_x']\n",
    "            input_df['error_from_ball_y'] = input_df['expected_y_at_ball'] - input_df['ball_land_y']\n",
    "            input_df['error_from_ball'] = np.sqrt(\n",
    "                input_df['error_from_ball_x']**2 + input_df['error_from_ball_y']**2\n",
    "            )\n",
    "            \n",
    "            input_df['weighted_dist_by_time'] = input_df['dist_to_ball'] / (input_df['frame_time'] + 0.1)\n",
    "            input_df['dist_scaled_by_progress'] = input_df['dist_to_ball'] * (1 - input_df['progress_ratio'])\n",
    "        \n",
    "        input_df['time_squared'] = input_df['frame_time'] ** 2\n",
    "        input_df['velocity_x_progress'] = input_df['velocity_x'] * input_df['progress_ratio']\n",
    "        input_df['velocity_y_progress'] = input_df['velocity_y'] * input_df['progress_ratio']\n",
    "        input_df['speed_scaled_by_time_left'] = input_df['s'] * input_df['time_remaining']\n",
    "        \n",
    "        input_df['actual_play_length'] = max_frames\n",
    "        input_df['length_ratio'] = max_frames / 30.0\n",
    "    \n",
    "    # ğŸ¯ THE BREAKTHROUGH: Add geometric features\n",
    "    print(\"Step 5: ğŸ¯ Geometric endpoint features...\")\n",
    "    input_df = add_geometric_features(input_df)\n",
    "    \n",
    "    print(\"Step 6: Building feature list...\")\n",
    "    \n",
    "    # Your 154 proven features\n",
    "    feature_cols = [\n",
    "        'x', 'y', 's', 'a', 'o', 'dir', 'frame_id', 'ball_land_x', 'ball_land_y',\n",
    "        'player_height_feet', 'player_weight', 'height_inches', 'bmi',\n",
    "        'velocity_x', 'velocity_y', 'acceleration_x', 'acceleration_y',\n",
    "        'momentum_x', 'momentum_y', 'kinetic_energy',\n",
    "        'speed_squared', 'accel_magnitude', 'orientation_diff',\n",
    "        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer',\n",
    "        'role_targeted_receiver', 'role_defensive_coverage', 'role_passer', 'side_offense',\n",
    "        'distance_to_ball', 'dist_to_ball', 'dist_squared', 'angle_to_ball', \n",
    "        'ball_direction_x', 'ball_direction_y', 'closing_speed_ball',\n",
    "        'velocity_toward_ball', 'velocity_alignment', 'angle_diff',\n",
    "        'nearest_opp_dist', 'closing_speed', 'num_nearby_opp_3', 'num_nearby_opp_5',\n",
    "        'mirror_wr_vx', 'mirror_wr_vy', 'mirror_offset_x', 'mirror_offset_y',\n",
    "        'pressure', 'under_pressure', 'pressure_x_speed', \n",
    "        'mirror_similarity', 'mirror_offset_dist', 'mirror_alignment',\n",
    "        'route_pattern', 'traj_straightness', 'traj_max_turn', 'traj_mean_turn',\n",
    "        'traj_depth', 'traj_width', 'speed_mean', 'speed_change',\n",
    "        'gnn_ally_dx_mean', 'gnn_ally_dy_mean', 'gnn_ally_dvx_mean', 'gnn_ally_dvy_mean',\n",
    "        'gnn_opp_dx_mean', 'gnn_opp_dy_mean', 'gnn_opp_dvx_mean', 'gnn_opp_dvy_mean',\n",
    "        'gnn_ally_cnt', 'gnn_opp_cnt',\n",
    "        'gnn_ally_dmin', 'gnn_ally_dmean', 'gnn_opp_dmin', 'gnn_opp_dmean',\n",
    "        'gnn_d1', 'gnn_d2', 'gnn_d3',\n",
    "    ]\n",
    "    \n",
    "    for lag in [1, 2, 3, 4, 5]:\n",
    "        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n",
    "            feature_cols.append(f'{col}_lag{lag}')\n",
    "    \n",
    "    for window in [3, 5]:\n",
    "        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n",
    "            feature_cols.append(f'{col}_rolling_mean_{window}')\n",
    "            feature_cols.append(f'{col}_rolling_std_{window}')\n",
    "    \n",
    "    feature_cols.extend(['velocity_x_delta', 'velocity_y_delta'])\n",
    "    feature_cols.extend(['velocity_x_ema', 'velocity_y_ema', 'speed_ema'])\n",
    "    \n",
    "    feature_cols.extend([\n",
    "        'max_play_duration', 'frame_time', 'progress_ratio', 'time_remaining', 'frames_remaining',\n",
    "        'expected_x_at_ball', 'expected_y_at_ball', \n",
    "        'error_from_ball_x', 'error_from_ball_y', 'error_from_ball',\n",
    "        'time_squared', 'weighted_dist_by_time', \n",
    "        'velocity_x_progress', 'velocity_y_progress', 'dist_scaled_by_progress',\n",
    "        'speed_scaled_by_time_left', 'actual_play_length', 'length_ratio',\n",
    "    ])\n",
    "    \n",
    "    # ğŸ¯ Add 13 geometric features\n",
    "    feature_cols.extend([\n",
    "        'geo_endpoint_x', 'geo_endpoint_y',\n",
    "        'geo_vector_x', 'geo_vector_y', 'geo_distance',\n",
    "        'geo_required_vx', 'geo_required_vy',\n",
    "        'geo_velocity_error_x', 'geo_velocity_error_y', 'geo_velocity_error',\n",
    "        'geo_required_ax', 'geo_required_ay',\n",
    "        'geo_alignment',\n",
    "    ])\n",
    "    \n",
    "    feature_cols = [c for c in feature_cols if c in input_df.columns]\n",
    "    print(f\"âœ“ Using {len(feature_cols)} features (154 proven + 13 geometric)\")\n",
    "    \n",
    "    print(\"Step 7: Creating sequences...\")\n",
    "    \n",
    "    input_df.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n",
    "    grouped = input_df.groupby(level=['game_id', 'play_id', 'nfl_id'])\n",
    "    \n",
    "    target_rows = output_df if is_training else test_template\n",
    "    target_groups = target_rows[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n",
    "    \n",
    "    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = [], [], [], [], []\n",
    "    geo_endpoints_x, geo_endpoints_y = [], []\n",
    "    \n",
    "    for _, row in tqdm(target_groups.iterrows(), total=len(target_groups), desc=\"Creating sequences\"):\n",
    "        key = (row['game_id'], row['play_id'], row['nfl_id'])\n",
    "        \n",
    "        try:\n",
    "            group_df = grouped.get_group(key)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "        input_window = group_df.tail(window_size)\n",
    "        \n",
    "        if len(input_window) < window_size:\n",
    "            if is_training:\n",
    "                continue\n",
    "            pad_len = window_size - len(input_window)\n",
    "            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n",
    "            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n",
    "        \n",
    "        input_window = input_window.fillna(group_df.mean(numeric_only=True))\n",
    "        seq = input_window[feature_cols].values\n",
    "        \n",
    "        if np.isnan(seq).any():\n",
    "            if is_training:\n",
    "                continue\n",
    "            seq = np.nan_to_num(seq, nan=0.0)\n",
    "        \n",
    "        sequences.append(seq)\n",
    "        \n",
    "        # Store geometric endpoint for this player\n",
    "        geo_x = input_window.iloc[-1]['geo_endpoint_x']\n",
    "        geo_y = input_window.iloc[-1]['geo_endpoint_y']\n",
    "        geo_endpoints_x.append(geo_x)\n",
    "        geo_endpoints_y.append(geo_y)\n",
    "        \n",
    "        if is_training:\n",
    "            out_grp = output_df[\n",
    "                (output_df['game_id']==row['game_id']) &\n",
    "                (output_df['play_id']==row['play_id']) &\n",
    "                (output_df['nfl_id']==row['nfl_id'])\n",
    "            ].sort_values('frame_id')\n",
    "            \n",
    "            last_x = input_window.iloc[-1]['x']\n",
    "            last_y = input_window.iloc[-1]['y']\n",
    "            \n",
    "            dx = out_grp['x'].values - last_x\n",
    "            dy = out_grp['y'].values - last_y\n",
    "            \n",
    "            targets_dx.append(dx)\n",
    "            targets_dy.append(dy)\n",
    "            targets_frame_ids.append(out_grp['frame_id'].values)\n",
    "        \n",
    "        sequence_ids.append({\n",
    "            'game_id': key[0],\n",
    "            'play_id': key[1],\n",
    "            'nfl_id': key[2],\n",
    "            'frame_id': input_window.iloc[-1]['frame_id']\n",
    "        })\n",
    "    \n",
    "    print(f\"âœ“ Created {len(sequences)} sequences\")\n",
    "    \n",
    "    if is_training:\n",
    "        return (sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, \n",
    "                geo_endpoints_x, geo_endpoints_y, route_kmeans, route_scaler)\n",
    "    return sequences, sequence_ids, geo_endpoints_x, geo_endpoints_y\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL ARCHITECTURE (YOUR PROVEN GRU + ATTENTION)\n",
    "# ============================================================================\n",
    "\n",
    "class JointSeqModel(nn.Module):\n",
    "    \"\"\"Your proven architecture - unchanged\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, horizon):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
    "        self.pool_ln = nn.LayerNorm(128)\n",
    "        self.pool_attn = nn.MultiheadAttention(128, num_heads=4, batch_first=True)\n",
    "        self.pool_query = nn.Parameter(torch.randn(1, 1, 128))\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(128, 256), \n",
    "            nn.GELU(), \n",
    "            nn.Dropout(0.2), \n",
    "            nn.Linear(256, horizon * 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h, _ = self.gru(x)\n",
    "        B = h.size(0)\n",
    "        q = self.pool_query.expand(B, -1, -1)\n",
    "        ctx, _ = self.pool_attn(q, self.pool_ln(h), self.pool_ln(h))\n",
    "        out = self.head(ctx.squeeze(1))\n",
    "        out = out.view(B, -1, 2)\n",
    "        return torch.cumsum(out, dim=1)\n",
    "\n",
    "# ============================================================================\n",
    "# LOSS (YOUR PROVEN TEMPORAL HUBER)\n",
    "# ============================================================================\n",
    "\n",
    "class TemporalHuber(nn.Module):\n",
    "    def __init__(self, delta=0.5, time_decay=0.03):\n",
    "        super().__init__()\n",
    "        self.delta = delta\n",
    "        self.time_decay = time_decay\n",
    "    \n",
    "    def forward(self, pred, target, mask):\n",
    "        err = pred - target\n",
    "        abs_err = torch.abs(err)\n",
    "        huber = torch.where(abs_err <= self.delta, 0.5 * err * err, \n",
    "                           self.delta * (abs_err - 0.5 * self.delta))\n",
    "        \n",
    "        if self.time_decay > 0:\n",
    "            L = pred.size(1)\n",
    "            t = torch.arange(L, device=pred.device).float()\n",
    "            weight = torch.exp(-self.time_decay * t).view(1, L, 1)\n",
    "            huber = huber * weight\n",
    "            mask = mask.unsqueeze(-1) * weight\n",
    "        \n",
    "        return (huber * mask).sum() / (mask.sum() + 1e-8)\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_targets(batch_dx, batch_dy, max_h):\n",
    "    tensors_x, tensors_y, masks = [], [], []\n",
    "    \n",
    "    for dx, dy in zip(batch_dx, batch_dy):\n",
    "        L = len(dx)\n",
    "        padded_x = np.pad(dx, (0, max_h - L), constant_values=0).astype(np.float32)\n",
    "        padded_y = np.pad(dy, (0, max_h - L), constant_values=0).astype(np.float32)\n",
    "        mask = np.zeros(max_h, dtype=np.float32)\n",
    "        mask[:L] = 1.0\n",
    "        \n",
    "        tensors_x.append(torch.tensor(padded_x))\n",
    "        tensors_y.append(torch.tensor(padded_y))\n",
    "        masks.append(torch.tensor(mask))\n",
    "    \n",
    "    targets = torch.stack([torch.stack(tensors_x), torch.stack(tensors_y)], dim=-1)\n",
    "    return targets, torch.stack(masks)\n",
    "\n",
    "def train_model(X_train, y_train_dx, y_train_dy, X_val, y_val_dx, y_val_dy, \n",
    "                input_dim, horizon, config):\n",
    "    device = config.DEVICE\n",
    "    model = JointSeqModel(input_dim, horizon).to(device)\n",
    "    \n",
    "    criterion = TemporalHuber(delta=0.5, time_decay=0.03)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5, verbose=False)\n",
    "    \n",
    "    train_batches = []\n",
    "    for i in range(0, len(X_train), config.BATCH_SIZE):\n",
    "        end = min(i + config.BATCH_SIZE, len(X_train))\n",
    "        bx = torch.tensor(np.stack(X_train[i:end]).astype(np.float32))\n",
    "        by, bm = prepare_targets(\n",
    "            [y_train_dx[j] for j in range(i, end)],\n",
    "            [y_train_dy[j] for j in range(i, end)],\n",
    "            horizon\n",
    "        )\n",
    "        train_batches.append((bx, by, bm))\n",
    "    \n",
    "    val_batches = []\n",
    "    for i in range(0, len(X_val), config.BATCH_SIZE):\n",
    "        end = min(i + config.BATCH_SIZE, len(X_val))\n",
    "        bx = torch.tensor(np.stack(X_val[i:end]).astype(np.float32))\n",
    "        by, bm = prepare_targets(\n",
    "            [y_val_dx[j] for j in range(i, end)],\n",
    "            [y_val_dy[j] for j in range(i, end)],\n",
    "            horizon\n",
    "        )\n",
    "        val_batches.append((bx, by, bm))\n",
    "    \n",
    "    best_loss, best_state, bad = float('inf'), None, 0\n",
    "    \n",
    "    for epoch in range(1, config.EPOCHS + 1):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for bx, by, bm in train_batches:\n",
    "            bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n",
    "            pred = model(bx)\n",
    "            loss = criterion(pred, by, bm)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_rmse_errors = []  # ğŸ¯ Track RMSE for competition metric\n",
    "        with torch.no_grad():\n",
    "            for bx, by, bm in val_batches:\n",
    "                bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n",
    "                pred = model(bx)\n",
    "                val_losses.append(criterion(pred, by, bm).item())\n",
    "                \n",
    "                # Calculate RMSE (competition metric)\n",
    "                masked_pred = pred * bm.unsqueeze(-1)\n",
    "                masked_target = by * bm.unsqueeze(-1)\n",
    "                squared_errors = (masked_pred - masked_target) ** 2\n",
    "                val_rmse_errors.append(squared_errors.sum().item())\n",
    "        \n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        \n",
    "        # ğŸ¯ Calculate RMSE: sqrt(sum of squared errors / (num_valid_points * 2 coords))\n",
    "        total_valid_points = sum(bm.sum().item() for _, _, bm in val_batches)\n",
    "        val_rmse = np.sqrt(sum(val_rmse_errors) / (total_valid_points * 2))\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"  Epoch {epoch}: train={train_loss:.4f}, val={val_loss:.4f} | RMSE={val_rmse:.4f}\")\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= config.PATIENCE:\n",
    "                print(f\"  Early stop at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    # Calculate final RMSE on validation set\n",
    "    model.eval()\n",
    "    final_rmse_errors = []\n",
    "    with torch.no_grad():\n",
    "        for bx, by, bm in val_batches:\n",
    "            bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n",
    "            pred = model(bx)\n",
    "            masked_pred = pred * bm.unsqueeze(-1)\n",
    "            masked_target = by * bm.unsqueeze(-1)\n",
    "            squared_errors = (masked_pred - masked_target) ** 2\n",
    "            final_rmse_errors.append(squared_errors.sum().item())\n",
    "    \n",
    "    total_valid_points = sum(bm.sum().item() for _, _, bm in val_batches)\n",
    "    final_rmse = np.sqrt(sum(final_rmse_errors) / (total_valid_points * 2))\n",
    "    \n",
    "    return model, best_loss, final_rmse\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "# def main():\n",
    "config = Config()\n",
    "\n",
    "print(\"\\n\" + \"ğŸ†\"*40)\n",
    "print(\"ğŸ†\" + \" \"*78 + \"ğŸ†\")\n",
    "print(\"ğŸ†  NFL BIG DATA BOWL 2026 - GEOMETRIC NEURAL BREAKTHROUGH     ğŸ†\")\n",
    "print(\"ğŸ†  Proven NN (0.59) + Geometric Insights (Our Discovery)      ğŸ†\")\n",
    "print(\"ğŸ†\" + \" \"*78 + \"ğŸ†\")\n",
    "print(\"ğŸ†\"*40 + \"\\n\")\n",
    "print(\"ğŸ¯ TARGET: 0.54-0.56 LB\\n\")\n",
    "\n",
    "# Load\n",
    "print(\"\\n[1/4] Loading data...\")\n",
    "train_input_files = [config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\n",
    "train_output_files = [config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n",
    "train_input = pd.concat([pd.read_csv(f) for f in train_input_files if f.exists()])\n",
    "train_output = pd.concat([pd.read_csv(f) for f in train_output_files if f.exists()])\n",
    "test_input = pd.read_csv(config.DATA_DIR / \"test_input.csv\")\n",
    "test_template = pd.read_csv(config.DATA_DIR / \"test.csv\")\n",
    "\n",
    "print(f\"âœ“ Train input: {train_input.shape}, Train output: {train_output.shape}\")\n",
    "print(f\"âœ“ Test input: {test_input.shape}, Test template: {test_template.shape}\")\n",
    "\n",
    "# NOTE: Update here to limit\n",
    "# first_plays = train_input[['game_id', 'play_id']].drop_duplicates().head(500)\n",
    "# train_input = train_input.merge(first_plays, on=['game_id', 'play_id'], how='inner')\n",
    "# train_output = train_output.merge(first_plays, on=['game_id', 'play_id'], how='inner')\n",
    "\n",
    "# ## ==== Prepare ====\n",
    "print(\"\\n[2/4] Preparing geometric sequences...\")\n",
    "result = prepare_sequences_geometric(\n",
    "    train_input, train_output, is_training=True, window_size=config.WINDOW_SIZE\n",
    ")\n",
    "sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, geo_x, geo_y, route_kmeans, route_scaler = result\n",
    "\n",
    "## === Save sequences ===\n",
    "export_dir = Path(\"/kaggle/working/bdb26\")\n",
    "export_dir.mkdir(exist_ok=True, parents=True)  # â† ADD THIS\n",
    "\n",
    "# Kmeans and route scaler\n",
    "with open(export_dir / \"route_kmeans.pkl\", 'wb') as f:\n",
    "    pickle.dump(route_kmeans, f)\n",
    "with open(export_dir / \"route_scaler.pkl\", 'wb') as f:\n",
    "    pickle.dump(route_scaler, f)\n",
    "\n",
    "# All other vars\n",
    "np.savez_compressed(\n",
    "    export_dir / \"training_sequences.npz\",\n",
    "    num_sequences=len(sequences),\n",
    "    sequences=np.array(sequences, dtype=object),\n",
    "    targets_dx=np.array(targets_dx, dtype=object),\n",
    "    targets_dy=np.array(targets_dy, dtype=object),\n",
    "    targets_frame_ids=np.array(targets_frame_ids, dtype=object),\n",
    "    sequence_ids=np.array(sequence_ids, dtype=object),\n",
    "    geo_endpoints_x=np.array(geo_x),\n",
    "    geo_endpoints_y=np.array(geo_y),\n",
    ")\n",
    "print(\"Saved output to kaggle working directory...\")\n",
    "\n",
    "# ======== This should work, but we don't need if everything works first time ==========\n",
    "data = np.load(\"/kaggle/working/training_sequences.npz\", allow_pickle=True)\n",
    "sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, geo_x, geo_y = map(\n",
    "    list, [data[k] for k in ['sequences', 'targets_dx', 'targets_dy', 'targets_frame_ids', 'sequence_ids', 'geo_endpoints_x', 'geo_endpoints_y']]\n",
    ")\n",
    "\n",
    "route_kmeans_path = export_dir / \"route_kmeans.pkl\"\n",
    "route_scaler_path = export_dir / \"route_scaler.pkl\"\n",
    "\n",
    "if route_kmeans_path.exists() and route_scaler_path.exists():\n",
    "    with open(route_kmeans_path, 'rb') as f:\n",
    "        route_kmeans = pickle.load(f)\n",
    "    with open(route_scaler_path, 'rb') as f:\n",
    "        route_scaler = pickle.load(f)\n",
    "    print(\"âœ“ Loaded route_kmeans and route_scaler\")\n",
    "else:\n",
    "    print(\"âš ï¸  route_kmeans/route_scaler not found in export_dir\")\n",
    "    route_kmeans = None\n",
    "    route_scaler = None\n",
    "print(\"Loaded variables from kaggle working directory\")\n",
    "# ======== This should work, but we don't need if everything works first time ==========\n",
    "\n",
    "sequences = list(sequences)\n",
    "targets_dx = list(targets_dx)\n",
    "targets_dy = list(targets_dy)\n",
    "\n",
    "# Train\n",
    "print(\"\\n[3/4] Training geometric models...\")\n",
    "groups = np.array([d['game_id'] for d in sequence_ids])\n",
    "gkf = GroupKFold(n_splits=config.N_FOLDS)\n",
    "\n",
    "models, scalers = [], []\n",
    "fold_rmses = []  # ğŸ¯ Track RMSE across folds\n",
    "\n",
    "for fold, (tr, va) in enumerate(gkf.split(sequences, groups=groups), 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Fold {fold}/{config.N_FOLDS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    X_tr = [sequences[i] for i in tr]\n",
    "    X_va = [sequences[i] for i in va]\n",
    "    y_tr_dx = [targets_dx[i] for i in tr]\n",
    "    y_va_dx = [targets_dx[i] for i in va]\n",
    "    y_tr_dy = [targets_dy[i] for i in tr]\n",
    "    y_va_dy = [targets_dy[i] for i in va]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.vstack([s for s in X_tr]))\n",
    "    \n",
    "    X_tr_sc = [scaler.transform(s) for s in X_tr]\n",
    "    X_va_sc = [scaler.transform(s) for s in X_va]\n",
    "    \n",
    "    model, loss, rmse = train_model(\n",
    "        X_tr_sc, y_tr_dx, y_tr_dy,\n",
    "        X_va_sc, y_va_dx, y_va_dy,\n",
    "        X_tr[0].shape[-1], config.MAX_FUTURE_HORIZON, config\n",
    "    )\n",
    "    \n",
    "    models.append(model)\n",
    "    scalers.append(scaler)\n",
    "    fold_rmses.append(rmse)\n",
    "    \n",
    "    print(f\"\\nâœ“ Fold {fold} - Huber Loss: {loss:.5f} | RMSE: {rmse:.4f}\")\n",
    "\n",
    "# ğŸ“Š Print summary statistics\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ“Š CROSS-VALIDATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mean RMSE: {np.mean(fold_rmses):.4f} Â± {np.std(fold_rmses):.4f}\")\n",
    "print(f\"Best RMSE: {np.min(fold_rmses):.4f}\")\n",
    "print(f\"Worst RMSE: {np.max(fold_rmses):.4f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# ğŸ’¾ EXPORT MODELS AND ARTIFACTS FOR LOCAL INFERENCE\n",
    "print(\"\\nğŸ’¾ Exporting models and artifacts...\")\n",
    "# export_dir = config.OUTPUT_DIR / \"trained_models\"\n",
    "# export_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save each fold's model and scaler\n",
    "for fold, (model, scaler) in enumerate(zip(models, scalers), 1):\n",
    "    model_path = export_dir / f\"model_fold{fold}.pth\"\n",
    "    scaler_path = export_dir / f\"scaler_fold{fold}.pkl\"\n",
    "    \n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    with open(scaler_path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    print(f\"  âœ“ Saved fold {fold}: {model_path.name}, {scaler_path.name}\")\n",
    "\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'n_folds': config.N_FOLDS,\n",
    "    'input_dim': X_tr[0].shape[-1],\n",
    "    'horizon': config.MAX_FUTURE_HORIZON,\n",
    "    'window_size': config.WINDOW_SIZE,\n",
    "    'fold_rmses': fold_rmses,\n",
    "    'mean_rmse': float(np.mean(fold_rmses)),\n",
    "    'std_rmse': float(np.std(fold_rmses))\n",
    "}\n",
    "with open(export_dir / \"metadata.pkl\", 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(f\"  âœ“ Saved route_kmeans.pkl, route_scaler.pkl, metadata.pkl\")\n",
    "print(f\"\\nğŸ‰ All artifacts exported to: {export_dir}\")\n",
    "print(f\"   Download this folder to run inference locally!\\n\")\n",
    "\n",
    "# # Test\n",
    "# print(\"\\n[4/4] Creating test predictions...\")\n",
    "# test_seq, test_ids, test_geo_x, test_geo_y = prepare_sequences_geometric(\n",
    "#     test_input, test_template=test_template, is_training=False, \n",
    "#     window_size=config.WINDOW_SIZE,\n",
    "#     route_kmeans=route_kmeans, route_scaler=route_scaler\n",
    "# )\n",
    "\n",
    "# X_test = list(test_seq)\n",
    "# x_last = np.array([s[-1, 0] for s in X_test])\n",
    "# y_last = np.array([s[-1, 1] for s in X_test])\n",
    "\n",
    "# # Ensemble\n",
    "# all_preds = []\n",
    "\n",
    "# for model, sc in zip(models, scalers):\n",
    "#     X_sc = [sc.transform(s) for s in X_test]\n",
    "#     X_t = torch.tensor(np.stack(X_sc).astype(np.float32)).to(config.DEVICE)\n",
    "    \n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         preds = model(X_t).cpu().numpy()\n",
    "    \n",
    "#     all_preds.append(preds)\n",
    "\n",
    "# ens_preds = np.mean(all_preds, axis=0)\n",
    "\n",
    "# # Submission\n",
    "# rows = []\n",
    "# H = ens_preds.shape[1]\n",
    "\n",
    "# for i, sid in enumerate(test_ids):\n",
    "#     fids = test_template[\n",
    "#         (test_template['game_id'] == sid['game_id']) &\n",
    "#         (test_template['play_id'] == sid['play_id']) &\n",
    "#         (test_template['nfl_id'] == sid['nfl_id'])\n",
    "#     ]['frame_id'].sort_values().tolist()\n",
    "    \n",
    "#     for t, fid in enumerate(fids):\n",
    "#         tt = min(t, H - 1)\n",
    "#         px = np.clip(x_last[i] + ens_preds[i, tt, 0], 0, 120)\n",
    "#         py = np.clip(y_last[i] + ens_preds[i, tt, 1], 0, 53.3)\n",
    "        \n",
    "#         rows.append({\n",
    "#             'id': f\"{sid['game_id']}_{sid['play_id']}_{sid['nfl_id']}_{fid}\",\n",
    "#             'x': px,\n",
    "#             'y': py\n",
    "#         })\n",
    "\n",
    "# submission = pd.DataFrame(rows)\n",
    "# submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "# print(\"\\n\" + \"ğŸ†\"*40)\n",
    "# print(\"ğŸ†  GEOMETRIC NEURAL BREAKTHROUGH COMPLETE!  ğŸ†\")\n",
    "# print(\"ğŸ†\"*40)\n",
    "# print(f\"\\nâœ“ Saved submission.csv ({len(submission)} rows)\")\n",
    "# print(f\"\\nğŸ“Š THE SYNTHESIS:\")\n",
    "# print(f\"  âœ“ 154 proven features (your 0.59 base)\")\n",
    "# print(f\"  âœ“ +13 geometric features (our discovery)\")\n",
    "# print(f\"  âœ“ = 167 total features\")\n",
    "# print(f\"\\nğŸ¯ Expected: 0.54-0.56 LB\")\n",
    "# print(f\"\\nğŸ’¡ Key insight: Model learns corrections to geometric baseline\")\n",
    "# print(f\"   - Geometry handles deterministic part (free)\")\n",
    "# print(f\"   - NN learns complex corrections (coverage, collisions)\")\n",
    "# print(f\"   - Less to learn = better generalization\")\n",
    "# print(\"\\n\" + \"ğŸ†\"*40 + \"\\n\")\n",
    "\n",
    "# return submission\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-10T02:45:36.444001Z",
     "iopub.status.idle": "2025-12-10T02:45:36.444235Z",
     "shell.execute_reply": "2025-12-10T02:45:36.444136Z",
     "shell.execute_reply.started": "2025-12-10T02:45:36.444125Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Config.DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14210809,
     "sourceId": 114239,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
