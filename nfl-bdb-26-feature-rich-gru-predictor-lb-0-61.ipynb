{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-12T17:21:35.173948Z",
     "iopub.status.busy": "2025-10-12T17:21:35.173419Z",
     "iopub.status.idle": "2025-10-12T18:08:04.841026Z",
     "shell.execute_reply": "2025-10-12T18:08:04.840206Z",
     "shell.execute_reply.started": "2025-10-12T17:21:35.173925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "COMPLETE STEP 2: Better Features Pipeline\n",
    "Full pipeline with advanced feature engineering\n",
    "\n",
    "Expected: 0.62 → 0.59-0.60 RMSE\n",
    "Time: ~4-5 hours (training)\n",
    "\n",
    "WHAT THIS DOES:\n",
    "1. Load and prepare data\n",
    "2. Add ADVANCED FEATURES (30-40 new features)\n",
    "   - Distance rate features\n",
    "   - Target alignment features\n",
    "   - Multi-window rolling features\n",
    "   - Extended lag features\n",
    "   - Velocity change features\n",
    "   - Field position features\n",
    "   - Role-specific features\n",
    "   - Time-based features\n",
    "3. Train NN models with enhanced features\n",
    "4. Create submission\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    DATA_DIR = Path(\"./data\")\n",
    "    OUTPUT_DIR = Path(\"./outputs\")\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    SEED = 42\n",
    "    N_FOLDS = 5\n",
    "    BATCH_SIZE = 128\n",
    "    EPOCHS = 200\n",
    "    PATIENCE = 20\n",
    "    LEARNING_RATE = 5e-4\n",
    "    \n",
    "    WINDOW_SIZE = 20\n",
    "    HIDDEN_DIM = 64\n",
    "    MAX_FUTURE_HORIZON = 94\n",
    "    \n",
    "    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n",
    "    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n",
    "    \n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(Config.SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# ENHANCED FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "def height_to_feet(height_str):\n",
    "    try:\n",
    "        ft, inches = map(int, str(height_str).split('-'))\n",
    "        return ft + inches/12\n",
    "    except:\n",
    "        return 6.0\n",
    "\n",
    "def add_advanced_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    STEP 2: Add 30-40 advanced features\n",
    "    These are proven to improve performance\n",
    "    \"\"\"\n",
    "    print(\"Adding advanced features...\")\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "    gcols = ['game_id', 'play_id', 'nfl_id']\n",
    "    \n",
    "    # ==========================================\n",
    "    # GROUP 1: Distance Rate Features (3)\n",
    "    # ==========================================\n",
    "    if 'distance_to_ball' in df.columns:\n",
    "        df['distance_to_ball_change'] = df.groupby(gcols)['distance_to_ball'].diff().fillna(0)\n",
    "        df['distance_to_ball_accel'] = df.groupby(gcols)['distance_to_ball_change'].diff().fillna(0)\n",
    "        df['time_to_intercept'] = (df['distance_to_ball'] / \n",
    "                                    (np.abs(df['distance_to_ball_change']) + 0.1)).clip(0, 10)\n",
    "    \n",
    "    # ==========================================\n",
    "    # GROUP 2: Target Alignment Features (3)\n",
    "    # ==========================================\n",
    "    if 'ball_direction_x' in df.columns:\n",
    "        df['velocity_alignment'] = (\n",
    "            df['velocity_x'] * df['ball_direction_x'] +\n",
    "            df['velocity_y'] * df['ball_direction_y']\n",
    "        )\n",
    "        df['velocity_perpendicular'] = (\n",
    "            df['velocity_x'] * (-df['ball_direction_y']) +\n",
    "            df['velocity_y'] * df['ball_direction_x']\n",
    "        )\n",
    "        if 'acceleration_x' in df.columns:\n",
    "            df['accel_alignment'] = (\n",
    "                df['acceleration_x'] * df['ball_direction_x'] +\n",
    "                df['acceleration_y'] * df['ball_direction_y']\n",
    "            )\n",
    "    \n",
    "    # ==========================================\n",
    "    # GROUP 3: Multi-Window Rolling (24)\n",
    "    # ==========================================\n",
    "    for window in [3, 5, 10]:\n",
    "        for col in ['velocity_x', 'velocity_y', 's', 'a']:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_roll{window}'] = df.groupby(gcols)[col].transform(\n",
    "                    lambda x: x.rolling(window, min_periods=1).mean()\n",
    "                )\n",
    "                df[f'{col}_std{window}'] = df.groupby(gcols)[col].transform(\n",
    "                    lambda x: x.rolling(window, min_periods=1).std()\n",
    "                ).fillna(0)\n",
    "    \n",
    "    # ==========================================\n",
    "    # GROUP 4: Extended Lag Features (8)\n",
    "    # ==========================================\n",
    "    for lag in [4, 5]:\n",
    "        for col in ['x', 'y', 'velocity_x', 'velocity_y']:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_lag{lag}'] = df.groupby(gcols)[col].shift(lag).fillna(0)\n",
    "    \n",
    "    # ==========================================\n",
    "    # GROUP 5: Velocity Change Features (4)\n",
    "    # ==========================================\n",
    "    if 'velocity_x' in df.columns:\n",
    "        df['velocity_x_change'] = df.groupby(gcols)['velocity_x'].diff().fillna(0)\n",
    "        df['velocity_y_change'] = df.groupby(gcols)['velocity_y'].diff().fillna(0)\n",
    "        df['speed_change'] = df.groupby(gcols)['s'].diff().fillna(0)\n",
    "        df['direction_change'] = df.groupby(gcols)['dir'].diff().fillna(0)\n",
    "        # df['direction_change'] = df['direction_change'].apply(\n",
    "        #     lambda x: x if abs(x) < 180 else x - 360 * np.sign(x)\n",
    "        # )\n",
    "        dir_diff = df.groupby(gcols)['dir'].diff().fillna(0)\n",
    "        df['direction_change'] = (((dir_diff + 180) % 360) - 180)\n",
    "    \n",
    "    # ==========================================\n",
    "    # GROUP 6: Field Position Features (4)\n",
    "    # ==========================================\n",
    "    df['dist_from_left'] = df['y']\n",
    "    df['dist_from_right'] = 53.3 - df['y']\n",
    "    df['dist_from_sideline'] = np.minimum(df['dist_from_left'], df['dist_from_right'])\n",
    "    df['dist_from_endzone'] = np.minimum(df['x'], 120 - df['x'])\n",
    "    \n",
    "    # ==========================================\n",
    "    # GROUP 7: Role-Specific Features (3)\n",
    "    # ==========================================\n",
    "    if 'is_receiver' in df.columns and 'velocity_alignment' in df.columns:\n",
    "        df['receiver_optimality'] = df['is_receiver'] * df['velocity_alignment']\n",
    "        df['receiver_deviation'] = df['is_receiver'] * np.abs(df.get('velocity_perpendicular', 0))\n",
    "    if 'is_coverage' in df.columns and 'closing_speed' in df.columns:\n",
    "        df['defender_closing_speed'] = df['is_coverage'] * df['closing_speed']\n",
    "    \n",
    "    # ==========================================\n",
    "    # GROUP 8: Time Features (2)\n",
    "    # ==========================================\n",
    "    df['frames_elapsed'] = df.groupby(gcols).cumcount()\n",
    "    df['normalized_time'] = df.groupby(gcols)['frames_elapsed'].transform(\n",
    "        lambda x: x / (x.max() + 1)\n",
    "    )\n",
    "    \n",
    "    print(f\"Total features after enhancement: {len(df.columns)}\")\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences_with_advanced_features(input_df: pd.DataFrame, \n",
    "                                             output_df: pd.DataFrame = None, \n",
    "                                             test_template: pd.DataFrame = None,\n",
    "                                             is_training: bool =True, \n",
    "                                             window_size: int = 8):\n",
    "    \"\"\"\n",
    "    Prepare sequences with ALL advanced features\n",
    "\n",
    "    Output:\n",
    "      - If is_training:\n",
    "          sequences: List of np.arrays (num_sequences, window_size, num_features)\n",
    "          targets_dx: List of np.arrays (num_sequences, num_future_frames)\n",
    "          targets_dy: List of np.arrays (num_sequences, num_future_frames)\n",
    "          targets_frame_ids: List of np.arrays (num_sequences, num_future_frames)\n",
    "          sequence_ids: List of sequence identifiers\n",
    "      - If not is_training:\n",
    "          sequences: List of np.arrays (num_sequences, window_size, num_features)\n",
    "          sequence_ids: List of sequence identifiers - game_id, play_id, nfl_id, frame_id\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PREPARING SEQUENCES WITH ADVANCED FEATURES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Window size: {window_size}\")\n",
    "    \n",
    "    input_df = input_df.copy()\n",
    "    \n",
    "    # ==========================================\n",
    "    # BASIC FEATURES\n",
    "    # ==========================================\n",
    "    print(\"Step 1/3: Adding basic features...\")\n",
    "    \n",
    "    input_df['player_height_feet'] = input_df['player_height'].apply(height_to_feet)\n",
    "    \n",
    "    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n",
    "    delta_t = 0.1\n",
    "    input_df['velocity_x'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.sin(dir_rad)\n",
    "    input_df['velocity_y'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.cos(dir_rad)\n",
    "    input_df['acceleration_x'] = input_df['a'] * np.sin(dir_rad)\n",
    "    input_df['acceleration_y'] = input_df['a'] * np.cos(dir_rad)\n",
    "    \n",
    "    # Roles\n",
    "    input_df['is_offense'] = (input_df['player_side'] == 'Offense').astype(int)\n",
    "    input_df['is_defense'] = (input_df['player_side'] == 'Defense').astype(int)\n",
    "    input_df['is_receiver'] = (input_df['player_role'] == 'Targeted Receiver').astype(int)\n",
    "    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n",
    "    input_df['is_passer'] = (input_df['player_role'] == 'Passer').astype(int)\n",
    "    \n",
    "    # Physics\n",
    "    mass_kg = input_df['player_weight'].fillna(200.0) / 2.20462\n",
    "    input_df['momentum_x'] = input_df['velocity_x'] * mass_kg\n",
    "    input_df['momentum_y'] = input_df['velocity_y'] * mass_kg\n",
    "    input_df['kinetic_energy'] = 0.5 * mass_kg * (input_df['s'] ** 2)\n",
    "    \n",
    "    # Ball features\n",
    "    if 'ball_land_x' in input_df.columns:\n",
    "        ball_dx = input_df['ball_land_x'] - input_df['x']\n",
    "        ball_dy = input_df['ball_land_y'] - input_df['y']\n",
    "        input_df['distance_to_ball'] = np.sqrt(ball_dx**2 + ball_dy**2)\n",
    "        input_df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n",
    "        input_df['ball_direction_x'] = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n",
    "        input_df['ball_direction_y'] = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n",
    "        input_df['closing_speed'] = (\n",
    "            input_df['velocity_x'] * input_df['ball_direction_x'] +\n",
    "            input_df['velocity_y'] * input_df['ball_direction_y']\n",
    "        )\n",
    "    \n",
    "    # Sort for temporal\n",
    "    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "    gcols = ['game_id', 'play_id', 'nfl_id']\n",
    "    \n",
    "    # Original lag features (1-3)\n",
    "    for lag in [1, 2, 3]:\n",
    "        input_df[f'x_lag{lag}'] = input_df.groupby(gcols)['x'].shift(lag)\n",
    "        input_df[f'y_lag{lag}'] = input_df.groupby(gcols)['y'].shift(lag)\n",
    "        input_df[f'velocity_x_lag{lag}'] = input_df.groupby(gcols)['velocity_x'].shift(lag)\n",
    "        input_df[f'velocity_y_lag{lag}'] = input_df.groupby(gcols)['velocity_y'].shift(lag)\n",
    "    \n",
    "    # EMA features\n",
    "    input_df['velocity_x_ema'] = input_df.groupby(gcols)['velocity_x'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "    input_df['velocity_y_ema'] = input_df.groupby(gcols)['velocity_y'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "    input_df['speed_ema'] = input_df.groupby(gcols)['s'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "    \n",
    "    # ==========================================\n",
    "    # ADVANCED FEATURES (NEW!)\n",
    "    # ==========================================\n",
    "    print(\"Step 2/3: Adding advanced features...\")\n",
    "    input_df = add_advanced_features(input_df)\n",
    "    \n",
    "    # ==========================================\n",
    "    # FEATURE LIST (ENHANCED)\n",
    "    # ==========================================\n",
    "    print(\"Step 3/3: Creating sequences...\")\n",
    "    \n",
    "    feature_cols = [\n",
    "        # Core (9)\n",
    "        'x', 'y', 's', 'a', 'o', 'dir', 'frame_id', 'ball_land_x', 'ball_land_y',\n",
    "        \n",
    "        # Player (2)\n",
    "        'player_height_feet', 'player_weight',\n",
    "        \n",
    "        # Motion (6)\n",
    "        'velocity_x', 'velocity_y', 'acceleration_x', 'acceleration_y',\n",
    "        'momentum_x', 'momentum_y', 'kinetic_energy',\n",
    "        \n",
    "        # Roles (5)\n",
    "        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer',\n",
    "        \n",
    "        # Ball (5)\n",
    "        'distance_to_ball', 'angle_to_ball', 'ball_direction_x', 'ball_direction_y', 'closing_speed',\n",
    "        \n",
    "        # Original temporal (15)\n",
    "        'x_lag1', 'y_lag1', 'velocity_x_lag1', 'velocity_y_lag1',\n",
    "        'x_lag2', 'y_lag2', 'velocity_x_lag2', 'velocity_y_lag2',\n",
    "        'x_lag3', 'y_lag3', 'velocity_x_lag3', 'velocity_y_lag3',\n",
    "        'velocity_x_ema', 'velocity_y_ema', 'speed_ema',\n",
    "        \n",
    "        # NEW: Distance rate (3)\n",
    "        'distance_to_ball_change', 'distance_to_ball_accel', 'time_to_intercept',\n",
    "        \n",
    "        # NEW: Target alignment (3)\n",
    "        'velocity_alignment', 'velocity_perpendicular', 'accel_alignment',\n",
    "        \n",
    "        # NEW: Multi-window rolling (24)\n",
    "        'velocity_x_roll3', 'velocity_x_std3', 'velocity_y_roll3', 'velocity_y_std3',\n",
    "        's_roll3', 's_std3', 'a_roll3', 'a_std3',\n",
    "        'velocity_x_roll5', 'velocity_x_std5', 'velocity_y_roll5', 'velocity_y_std5',\n",
    "        's_roll5', 's_std5', 'a_roll5', 'a_std5',\n",
    "        'velocity_x_roll10', 'velocity_x_std10', 'velocity_y_roll10', 'velocity_y_std10',\n",
    "        's_roll10', 's_std10', 'a_roll10', 'a_std10',\n",
    "        \n",
    "        # NEW: Extended lags (8)\n",
    "        'x_lag4', 'y_lag4', 'velocity_x_lag4', 'velocity_y_lag4',\n",
    "        'x_lag5', 'y_lag5', 'velocity_x_lag5', 'velocity_y_lag5',\n",
    "        \n",
    "        # NEW: Velocity changes (4)\n",
    "        'velocity_x_change', 'velocity_y_change', 'speed_change', 'direction_change',\n",
    "        \n",
    "        # NEW: Field position (4)\n",
    "        'dist_from_sideline', 'dist_from_endzone',\n",
    "        \n",
    "        # NEW: Role-specific (3)\n",
    "        'receiver_optimality', 'receiver_deviation', 'defender_closing_speed',\n",
    "        \n",
    "        # NEW: Time (2)\n",
    "        'frames_elapsed', 'normalized_time',\n",
    "    ]\n",
    "    \n",
    "    # Filter to existing\n",
    "    feature_cols = [c for c in feature_cols if c in input_df.columns]\n",
    "    print(f\"Using {len(feature_cols)} features (was ~50, now ~90)\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # CREATE SEQUENCES\n",
    "    # ==========================================\n",
    "    input_df.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n",
    "    grouped = input_df.groupby(level=['game_id', 'play_id', 'nfl_id'])\n",
    "    \n",
    "    target_rows = output_df if is_training else test_template # This is \"output\" for training or test set\n",
    "    target_groups = target_rows[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n",
    "    \n",
    "    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = [], [], [], [], []\n",
    "    \n",
    "    for _, row in tqdm(target_groups.iterrows(), total=len(target_groups), desc=\"Creating sequences\"):\n",
    "        key = (row['game_id'], row['play_id'], row['nfl_id'])\n",
    "        \n",
    "        try:\n",
    "            group_df = grouped.get_group(key)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "        input_window = group_df.tail(window_size)\n",
    "        \n",
    "        if len(input_window) < window_size:\n",
    "            if is_training:\n",
    "                continue\n",
    "            pad_len = window_size - len(input_window)\n",
    "            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n",
    "            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n",
    "        \n",
    "        input_window = input_window.fillna(group_df.mean(numeric_only=True))\n",
    "        seq = input_window[feature_cols].values\n",
    "        \n",
    "        if np.isnan(seq).any():\n",
    "            if is_training:\n",
    "                continue\n",
    "            seq = np.nan_to_num(seq, nan=0.0)\n",
    "        \n",
    "        sequences.append(seq)\n",
    "        \n",
    "        if is_training:\n",
    "            out_grp = output_df[\n",
    "                (output_df['game_id']==row['game_id']) &\n",
    "                (output_df['play_id']==row['play_id']) &\n",
    "                (output_df['nfl_id']==row['nfl_id'])\n",
    "            ].sort_values('frame_id')\n",
    "            \n",
    "            last_x = input_window.iloc[-1]['x']\n",
    "            last_y = input_window.iloc[-1]['y']\n",
    "            \n",
    "            dx = out_grp['x'].values - last_x\n",
    "            dy = out_grp['y'].values - last_y\n",
    "            \n",
    "            targets_dx.append(dx)\n",
    "            targets_dy.append(dy)\n",
    "            targets_frame_ids.append(out_grp['frame_id'].values)\n",
    "        \n",
    "        sequence_ids.append({\n",
    "            'game_id': key[0],\n",
    "            'play_id': key[1],\n",
    "            'nfl_id': key[2],\n",
    "            'frame_id': input_window.iloc[-1]['frame_id']\n",
    "        })\n",
    "    \n",
    "    print(f\"Created {len(sequences)} sequences with {len(feature_cols)} features each\")\n",
    "    \n",
    "    if is_training:\n",
    "        return sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids\n",
    "    return sequences, sequence_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL & LOSS (Same as before)\n",
    "# ============================================================================\n",
    "\n",
    "class TemporalHuber(nn.Module):\n",
    "    def __init__(self, delta=0.5, time_decay=0.03):\n",
    "        super().__init__()\n",
    "        self.delta = delta\n",
    "        self.time_decay = time_decay\n",
    "    \n",
    "    def forward(self, pred, target, mask):\n",
    "        err = pred - target\n",
    "        abs_err = torch.abs(err)\n",
    "        huber = torch.where(abs_err <= self.delta, 0.5 * err * err, \n",
    "                           self.delta * (abs_err - 0.5 * self.delta))\n",
    "        \n",
    "        if self.time_decay > 0:\n",
    "            L = pred.size(1)\n",
    "            t = torch.arange(L, device=pred.device).float()\n",
    "            weight = torch.exp(-self.time_decay * t).view(1, L)\n",
    "            huber, mask = huber * weight, mask * weight\n",
    "        \n",
    "        return (huber * mask).sum() / (mask.sum() + 1e-8)\n",
    "\n",
    "class SeqModel(nn.Module):\n",
    "    def __init__(self, input_dim, horizon):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
    "        self.pool_ln = nn.LayerNorm(128)\n",
    "        self.pool_attn = nn.MultiheadAttention(128, num_heads=4, batch_first=True)\n",
    "        self.pool_query = nn.Parameter(torch.randn(1, 1, 128))\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(128, 128), nn.GELU(), nn.Dropout(0.2), nn.Linear(128, horizon)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h, _ = self.gru(x)\n",
    "        B = h.size(0)\n",
    "        q = self.pool_query.expand(B, -1, -1)\n",
    "        ctx, _ = self.pool_attn(q, self.pool_ln(h), self.pool_ln(h))\n",
    "        out = self.head(ctx.squeeze(1))\n",
    "        return torch.cumsum(out, dim=1)\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_targets(batch_axis, max_h):\n",
    "    tensors, masks = [], []\n",
    "    for arr in batch_axis:\n",
    "        L = len(arr)\n",
    "        padded = np.pad(arr, (0, max_h - L), constant_values=0).astype(np.float32)\n",
    "        mask = np.zeros(max_h, dtype=np.float32)\n",
    "        mask[:L] = 1.0\n",
    "        tensors.append(torch.tensor(padded))\n",
    "        masks.append(torch.tensor(mask))\n",
    "    return torch.stack(tensors), torch.stack(masks)\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val, input_dim, horizon, config):\n",
    "    device = config.DEVICE\n",
    "    model = SeqModel(input_dim, horizon).to(device)\n",
    "    \n",
    "    criterion = TemporalHuber(delta=0.5, time_decay=0.03)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    \n",
    "    # Batches\n",
    "    train_batches = []\n",
    "    for i in range(0, len(X_train), config.BATCH_SIZE):\n",
    "        end = min(i + config.BATCH_SIZE, len(X_train))\n",
    "        bx = torch.tensor(np.stack(X_train[i:end]).astype(np.float32))\n",
    "        by, bm = prepare_targets([y_train[j] for j in range(i, end)], horizon)\n",
    "        train_batches.append((bx, by, bm))\n",
    "    \n",
    "    val_batches = []\n",
    "    for i in range(0, len(X_val), config.BATCH_SIZE):\n",
    "        end = min(i + config.BATCH_SIZE, len(X_val))\n",
    "        bx = torch.tensor(np.stack(X_val[i:end]).astype(np.float32))\n",
    "        by, bm = prepare_targets([y_val[j] for j in range(i, end)], horizon)\n",
    "        val_batches.append((bx, by, bm))\n",
    "    \n",
    "    best_loss, best_state, bad = float('inf'), None, 0\n",
    "    \n",
    "    for epoch in tqdm(range(1, config.EPOCHS + 1)):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for bx, by, bm in train_batches:\n",
    "            bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n",
    "            pred = model(bx)\n",
    "            loss = criterion(pred, by, bm)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for bx, by, bm in val_batches:\n",
    "                bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n",
    "                pred = model(bx)\n",
    "                val_losses.append(criterion(pred, by, bm).item())\n",
    "        \n",
    "        train_loss, val_loss = np.mean(train_losses), np.mean(val_losses)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"  Epoch {epoch}: train={train_loss:.4f}, val={val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= config.PATIENCE:\n",
    "                print(f\"  Early stop at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    return model, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-12T17:21:35.173948Z",
     "iopub.status.busy": "2025-10-12T17:21:35.173419Z",
     "iopub.status.idle": "2025-10-12T18:08:04.841026Z",
     "shell.execute_reply": "2025-10-12T18:08:04.840206Z",
     "shell.execute_reply.started": "2025-10-12T17:21:35.173925Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 2: BETTER FEATURES PIPELINE\n",
      "================================================================================\n",
      "\n",
      "Adding 30-40 advanced features to improve from 0.62 → 0.59-0.60\n",
      "\n",
      "[1/4] Loading data...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 2: BETTER FEATURES PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAdding 30-40 advanced features to improve from 0.62 → 0.59-0.60\")\n",
    "\n",
    "# Load\n",
    "print(\"\\n[1/4] Loading data...\")\n",
    "train_input_files = [config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\n",
    "train_output_files = [config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n",
    "train_input = pd.concat([pd.read_csv(f) for f in train_input_files if f.exists()])\n",
    "train_output = pd.concat([pd.read_csv(f) for f in train_output_files if f.exists()])\n",
    "test_input = pd.read_csv(config.DATA_DIR / \"test_input.csv\")\n",
    "test_template = pd.read_csv(config.DATA_DIR / \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/4] Preparing with ADVANCED features...\n",
      "\n",
      "================================================================================\n",
      "PREPARING SEQUENCES WITH ADVANCED FEATURES\n",
      "================================================================================\n",
      "Window size: 20\n",
      "Step 1/3: Adding basic features...\n",
      "Step 2/3: Adding advanced features...\n",
      "Adding advanced features...\n",
      "Total features after enhancement: 107\n",
      "Step 3/3: Creating sequences...\n",
      "Using 92 features (was ~50, now ~90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating sequences: 100%|██████████| 46045/46045 [05:25<00:00, 141.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 40272 sequences with 92 features each\n",
      "\n",
      "[3/4] Training with enhanced features...\n",
      "\n",
      "============================================================\n",
      "Fold 1/5\n",
      "============================================================\n",
      "Training X-axis model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Train X\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining X-axis model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m mx, loss_x = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_tr_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_dx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_va_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_dx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mva\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMAX_FUTURE_HORIZON\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Train Y\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining Y-axis model...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(X_train, y_train, X_val, y_val, input_dim, horizon, config)\u001b[39m\n\u001b[32m     63\u001b[39m criterion = TemporalHuber(delta=\u001b[32m0.5\u001b[39m, time_decay=\u001b[32m0.03\u001b[39m)\n\u001b[32m     64\u001b[39m optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=\u001b[32m1e-5\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m scheduler = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Batches\u001b[39;00m\n\u001b[32m     68\u001b[39m train_batches = []\n",
      "\u001b[31mTypeError\u001b[39m: ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "# Prepare with advanced features\n",
    "print(\"\\n[2/4] Preparing with ADVANCED features...\")\n",
    "sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = prepare_sequences_with_advanced_features(\n",
    "    train_input, train_output, is_training=True, window_size=config.WINDOW_SIZE\n",
    ")\n",
    " \n",
    "sequences = np.array(sequences, dtype=object)\n",
    "targets_dx = np.array(targets_dx, dtype=object)\n",
    "targets_dy = np.array(targets_dy, dtype=object)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/4] Training with enhanced features...\n",
      "\n",
      "============================================================\n",
      "Fold 1/5\n",
      "============================================================\n",
      "Training X-axis model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/200 [00:22<37:54, 11.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Train X\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining X-axis model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m mx, loss_x = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_tr_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_dx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_va_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_dx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mva\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMAX_FUTURE_HORIZON\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Train Y\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining Y-axis model...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(X_train, y_train, X_val, y_val, input_dim, horizon, config)\u001b[39m\n\u001b[32m     90\u001b[39m loss = criterion(pred, by, bm)\n\u001b[32m     91\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m     94\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/bdb-26/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "print(\"\\n[3/4] Training with enhanced features...\")\n",
    "groups = np.array([d['game_id'] for d in sequence_ids])\n",
    "gkf = GroupKFold(n_splits=config.N_FOLDS)\n",
    "\n",
    "models_x, models_y, scalers = [], [], []\n",
    "\n",
    "for fold, (tr, va) in enumerate(gkf.split(sequences, groups=groups), 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Fold {fold}/{config.N_FOLDS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    X_tr, X_va = sequences[tr], sequences[va]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.vstack([s for s in X_tr]))\n",
    "    \n",
    "    X_tr_sc = np.stack([scaler.transform(s) for s in X_tr])\n",
    "    X_va_sc = np.stack([scaler.transform(s) for s in X_va])\n",
    "    \n",
    "    # Train X\n",
    "    print(\"Training X-axis model...\")\n",
    "    mx, loss_x = train_model(\n",
    "        X_tr_sc, targets_dx[tr], X_va_sc, targets_dx[va],\n",
    "        X_tr[0].shape[-1], config.MAX_FUTURE_HORIZON, config\n",
    "    )\n",
    "    \n",
    "    # Train Y\n",
    "    print(\"Training Y-axis model...\")\n",
    "    my, loss_y = train_model(\n",
    "        X_tr_sc, targets_dy[tr], X_va_sc, targets_dy[va],\n",
    "        X_tr[0].shape[-1], config.MAX_FUTURE_HORIZON, config\n",
    "    )\n",
    "    \n",
    "    models_x.append(mx)\n",
    "    models_y.append(my)\n",
    "    scalers.append(scaler)\n",
    "    \n",
    "    print(f\"\\nFold {fold} - X loss: {loss_x:.5f}, Y loss: {loss_y:.5f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "print(\"\\n[4/4] Creating test predictions...\")\n",
    "test_sequences, test_ids = prepare_sequences_with_advanced_features(\n",
    "    test_input, test_template=test_template, is_training=False, window_size=config.WINDOW_SIZE\n",
    ")\n",
    "\n",
    "X_test = np.array(test_sequences, dtype=object)\n",
    "x_last = np.array([s[-1, 0] for s in X_test])\n",
    "y_last = np.array([s[-1, 1] for s in X_test])\n",
    "\n",
    "# Ensemble predictions across folds\n",
    "all_dx, all_dy = [], []\n",
    "for mx, my, sc in zip(models_x, models_y, scalers):\n",
    "    X_sc = np.stack([sc.transform(s) for s in X_test])\n",
    "    X_t = torch.tensor(X_sc.astype(np.float32)).to(config.DEVICE)\n",
    "    \n",
    "    mx.eval()\n",
    "    my.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        all_dx.append(mx(X_t).cpu().numpy())\n",
    "        all_dy.append(my(X_t).cpu().numpy())\n",
    "\n",
    "ens_dx = np.mean(all_dx, axis=0)\n",
    "ens_dy = np.mean(all_dy, axis=0)\n",
    "\n",
    "# Create submission\n",
    "rows = []\n",
    "H = ens_dx.shape[1]\n",
    "\n",
    "for i, sid in enumerate(test_ids):\n",
    "    fids = test_template[\n",
    "        (test_template['game_id'] == sid['game_id']) &\n",
    "        (test_template['play_id'] == sid['play_id']) &\n",
    "        (test_template['nfl_id'] == sid['nfl_id'])\n",
    "    ]['frame_id'].sort_values().tolist()\n",
    "    \n",
    "    for t, fid in enumerate(fids):\n",
    "        tt = min(t, H - 1)\n",
    "        px = np.clip(x_last[i] + ens_dx[i, tt], 0, 120)\n",
    "        py = np.clip(y_last[i] + ens_dy[i, tt], 0, 53.3)\n",
    "        \n",
    "        rows.append({\n",
    "            'id': f\"{sid['game_id']}_{sid['play_id']}_{sid['nfl_id']}_{fid}\",\n",
    "            'x': px,\n",
    "            'y': py\n",
    "        })\n",
    "\n",
    "submission = pd.DataFrame(rows)\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2 COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Saved submission.csv\")\n",
    "print(f\"  Rows: {len(submission)}\")\n",
    "print(f\"  Features used: ~90 (was ~50)\")\n",
    "print(f\"\\nExpected improvement:\")\n",
    "print(f\"  Before: 0.62 RMSE (baseline features)\")\n",
    "print(f\"  After:  0.59-0.60 RMSE (with advanced features)\")\n",
    "print(f\"\\nNew feature groups added:\")\n",
    "print(f\"  1. Distance rate features (3)\")\n",
    "print(f\"  2. Target alignment features (3)\")\n",
    "print(f\"  3. Multi-window rolling features (24)\")\n",
    "print(f\"  4. Extended lag features (8)\")\n",
    "print(f\"  5. Velocity change features (4)\")\n",
    "print(f\"  6. Field position features (4)\")\n",
    "print(f\"  7. Role-specific features (3)\")\n",
    "print(f\"  8. Time-based features (2)\")\n",
    "print(f\"\\nTotal new features: ~40\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = train_input, \n",
    "output_df = train_output\n",
    "is_training=True\n",
    "window_size=config.WINDOW_SIZE\n",
    "\n",
    "\"\"\"\n",
    "Prepare sequences with ALL advanced features\n",
    "\"\"\"\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"PREPARING SEQUENCES WITH ADVANCED FEATURES\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Window size: {window_size}\")\n",
    "\n",
    "input_df = input_df.copy()\n",
    "\n",
    "# ==========================================\n",
    "# BASIC FEATURES\n",
    "# ==========================================\n",
    "print(\"Step 1/3: Adding basic features...\")\n",
    "\n",
    "input_df['player_height_feet'] = input_df['player_height'].apply(height_to_feet)\n",
    "\n",
    "dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n",
    "delta_t = 0.1\n",
    "input_df['velocity_x'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.sin(dir_rad)\n",
    "input_df['velocity_y'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.cos(dir_rad)\n",
    "input_df['acceleration_x'] = input_df['a'] * np.sin(dir_rad)\n",
    "input_df['acceleration_y'] = input_df['a'] * np.cos(dir_rad)\n",
    "\n",
    "# Roles\n",
    "input_df['is_offense'] = (input_df['player_side'] == 'Offense').astype(int)\n",
    "input_df['is_defense'] = (input_df['player_side'] == 'Defense').astype(int)\n",
    "input_df['is_receiver'] = (input_df['player_role'] == 'Targeted Receiver').astype(int)\n",
    "input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n",
    "input_df['is_passer'] = (input_df['player_role'] == 'Passer').astype(int)\n",
    "\n",
    "# Physics\n",
    "mass_kg = input_df['player_weight'].fillna(200.0) / 2.20462\n",
    "input_df['momentum_x'] = input_df['velocity_x'] * mass_kg\n",
    "input_df['momentum_y'] = input_df['velocity_y'] * mass_kg\n",
    "input_df['kinetic_energy'] = 0.5 * mass_kg * (input_df['s'] ** 2)\n",
    "\n",
    "# Ball features\n",
    "if 'ball_land_x' in input_df.columns:\n",
    "    ball_dx = input_df['ball_land_x'] - input_df['x']\n",
    "    ball_dy = input_df['ball_land_y'] - input_df['y']\n",
    "    input_df['distance_to_ball'] = np.sqrt(ball_dx**2 + ball_dy**2)\n",
    "    input_df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n",
    "    input_df['ball_direction_x'] = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n",
    "    input_df['ball_direction_y'] = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n",
    "    input_df['closing_speed'] = (\n",
    "        input_df['velocity_x'] * input_df['ball_direction_x'] +\n",
    "        input_df['velocity_y'] * input_df['ball_direction_y']\n",
    "    )\n",
    "\n",
    "# Sort for temporal\n",
    "input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "gcols = ['game_id', 'play_id', 'nfl_id']\n",
    "\n",
    "# Original lag features (1-3)\n",
    "for lag in [1, 2, 3]:\n",
    "    input_df[f'x_lag{lag}'] = input_df.groupby(gcols)['x'].shift(lag)\n",
    "    input_df[f'y_lag{lag}'] = input_df.groupby(gcols)['y'].shift(lag)\n",
    "    input_df[f'velocity_x_lag{lag}'] = input_df.groupby(gcols)['velocity_x'].shift(lag)\n",
    "    input_df[f'velocity_y_lag{lag}'] = input_df.groupby(gcols)['velocity_y'].shift(lag)\n",
    "\n",
    "# EMA features\n",
    "input_df['velocity_x_ema'] = input_df.groupby(gcols)['velocity_x'].transform(\n",
    "    lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    ")\n",
    "input_df['velocity_y_ema'] = input_df.groupby(gcols)['velocity_y'].transform(\n",
    "    lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    ")\n",
    "input_df['speed_ema'] = input_df.groupby(gcols)['s'].transform(\n",
    "    lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# ADVANCED FEATURES (NEW!)\n",
    "# ==========================================\n",
    "print(\"Step 2/3: Adding advanced features...\")\n",
    "input_df = add_advanced_features(input_df)\n",
    "\n",
    "# ==========================================\n",
    "# FEATURE LIST (ENHANCED)\n",
    "# ==========================================\n",
    "print(\"Step 3/3: Creating sequences...\")\n",
    "\n",
    "feature_cols = [\n",
    "    # Core (9)\n",
    "    'x', 'y', 's', 'a', 'o', 'dir', 'frame_id', 'ball_land_x', 'ball_land_y',\n",
    "    \n",
    "    # Player (2)\n",
    "    'player_height_feet', 'player_weight',\n",
    "    \n",
    "    # Motion (6)\n",
    "    'velocity_x', 'velocity_y', 'acceleration_x', 'acceleration_y',\n",
    "    'momentum_x', 'momentum_y', 'kinetic_energy',\n",
    "    \n",
    "    # Roles (5)\n",
    "    'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer',\n",
    "    \n",
    "    # Ball (5)\n",
    "    'distance_to_ball', 'angle_to_ball', 'ball_direction_x', 'ball_direction_y', 'closing_speed',\n",
    "    \n",
    "    # Original temporal (15)\n",
    "    'x_lag1', 'y_lag1', 'velocity_x_lag1', 'velocity_y_lag1',\n",
    "    'x_lag2', 'y_lag2', 'velocity_x_lag2', 'velocity_y_lag2',\n",
    "    'x_lag3', 'y_lag3', 'velocity_x_lag3', 'velocity_y_lag3',\n",
    "    'velocity_x_ema', 'velocity_y_ema', 'speed_ema',\n",
    "    \n",
    "    # NEW: Distance rate (3)\n",
    "    'distance_to_ball_change', 'distance_to_ball_accel', 'time_to_intercept',\n",
    "    \n",
    "    # NEW: Target alignment (3)\n",
    "    'velocity_alignment', 'velocity_perpendicular', 'accel_alignment',\n",
    "    \n",
    "    # NEW: Multi-window rolling (24)\n",
    "    'velocity_x_roll3', 'velocity_x_std3', 'velocity_y_roll3', 'velocity_y_std3',\n",
    "    's_roll3', 's_std3', 'a_roll3', 'a_std3',\n",
    "    'velocity_x_roll5', 'velocity_x_std5', 'velocity_y_roll5', 'velocity_y_std5',\n",
    "    's_roll5', 's_std5', 'a_roll5', 'a_std5',\n",
    "    'velocity_x_roll10', 'velocity_x_std10', 'velocity_y_roll10', 'velocity_y_std10',\n",
    "    's_roll10', 's_std10', 'a_roll10', 'a_std10',\n",
    "    \n",
    "    # NEW: Extended lags (8)\n",
    "    'x_lag4', 'y_lag4', 'velocity_x_lag4', 'velocity_y_lag4',\n",
    "    'x_lag5', 'y_lag5', 'velocity_x_lag5', 'velocity_y_lag5',\n",
    "    \n",
    "    # NEW: Velocity changes (4)\n",
    "    'velocity_x_change', 'velocity_y_change', 'speed_change', 'direction_change',\n",
    "    \n",
    "    # NEW: Field position (4)\n",
    "    'dist_from_sideline', 'dist_from_endzone',\n",
    "    \n",
    "    # NEW: Role-specific (3)\n",
    "    'receiver_optimality', 'receiver_deviation', 'defender_closing_speed',\n",
    "    \n",
    "    # NEW: Time (2)\n",
    "    'frames_elapsed', 'normalized_time',\n",
    "]\n",
    "\n",
    "# Filter to existing\n",
    "feature_cols = [c for c in feature_cols if c in input_df.columns]\n",
    "print(f\"Using {len(feature_cols)} features (was ~50, now ~90)\")\n",
    "\n",
    "# ==========================================\n",
    "# CREATE SEQUENCES\n",
    "# ==========================================\n",
    "input_df.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n",
    "grouped = input_df.groupby(level=['game_id', 'play_id', 'nfl_id'])\n",
    "\n",
    "target_rows = output_df if is_training else test_template\n",
    "target_groups = target_rows[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n",
    "\n",
    "sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = [], [], [], [], []\n",
    "\n",
    "for _, row in tqdm(target_groups.iterrows(), total=len(target_groups), desc=\"Creating sequences\"):\n",
    "    key = (row['game_id'], row['play_id'], row['nfl_id'])\n",
    "    \n",
    "    try:\n",
    "        group_df = grouped.get_group(key)\n",
    "    except KeyError:\n",
    "        continue\n",
    "    \n",
    "    input_window = group_df.tail(window_size)\n",
    "    \n",
    "    if len(input_window) < window_size:\n",
    "        if is_training:\n",
    "            continue\n",
    "        pad_len = window_size - len(input_window)\n",
    "        pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n",
    "        input_window = pd.concat([pad_df, input_window], ignore_index=True)\n",
    "    \n",
    "    input_window = input_window.fillna(group_df.mean(numeric_only=True))\n",
    "    seq = input_window[feature_cols].values\n",
    "    \n",
    "    if np.isnan(seq).any():\n",
    "        if is_training:\n",
    "            continue\n",
    "        seq = np.nan_to_num(seq, nan=0.0)\n",
    "    \n",
    "    sequences.append(seq)\n",
    "    \n",
    "    if is_training:\n",
    "        out_grp = output_df[\n",
    "            (output_df['game_id']==row['game_id']) &\n",
    "            (output_df['play_id']==row['play_id']) &\n",
    "            (output_df['nfl_id']==row['nfl_id'])\n",
    "        ].sort_values('frame_id')\n",
    "        \n",
    "        last_x = input_window.iloc[-1]['x']\n",
    "        last_y = input_window.iloc[-1]['y']\n",
    "        \n",
    "        dx = out_grp['x'].values - last_x\n",
    "        dy = out_grp['y'].values - last_y\n",
    "        \n",
    "        targets_dx.append(dx)\n",
    "        targets_dy.append(dy)\n",
    "        targets_frame_ids.append(out_grp['frame_id'].values)\n",
    "    \n",
    "    sequence_ids.append({\n",
    "        'game_id': key[0],\n",
    "        'play_id': key[1],\n",
    "        'nfl_id': key[2],\n",
    "        'frame_id': input_window.iloc[-1]['frame_id']\n",
    "    })\n",
    "\n",
    "print(f\"Created {len(sequences)} sequences with {len(feature_cols)} features each\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14210809,
     "sourceId": 114239,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
